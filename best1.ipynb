{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa8c7e03",
      "metadata": {
        "id": "fa8c7e03"
      },
      "source": [
        "# MGTF 424 Final Project\n",
        "This notebook builds a baseline model to predict `return_on_asset` using panel data with anonymized indicators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5140c0d2",
      "metadata": {
        "id": "5140c0d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "sample_submission_df = pd.read_csv(\"sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4edc1d69",
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Remove outliers from train_df based on return_on_asset (e.g., 1st and 99th percentiles)\n",
        "# q_low = train_df['return_on_asset'].quantile(0.01)\n",
        "# q_high = train_df['return_on_asset'].quantile(0.99)\n",
        "# train_df_clean = train_df[(train_df['return_on_asset'] >= q_low) & (train_df['return_on_asset'] <= q_high)].copy()\n",
        "\n",
        "# # Normalize target\n",
        "# scaler = StandardScaler()\n",
        "# train_df_clean['return_on_asset_norm'] = scaler.fit_transform(train_df_clean[['return_on_asset']])\n",
        "\n",
        "# # Add lag features and rolling stats for each asset_id\n",
        "# lag_features = []\n",
        "# rolling_features = []\n",
        "# window = 3\n",
        "\n",
        "# for col in [c for c in train_df_clean.columns if c.startswith('indicator_')]:\n",
        "#     # Lag 1\n",
        "#     lag_col = f\"{col}_lag1\"\n",
        "#     train_df_clean[lag_col] = train_df_clean.groupby('asset_id')[col].shift(1)\n",
        "#     lag_features.append(lag_col)\n",
        "#     # Rolling mean\n",
        "#     roll_mean_col = f\"{col}_roll{window}_mean\"\n",
        "#     train_df_clean[roll_mean_col] = train_df_clean.groupby('asset_id')[col].rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "#     rolling_features.append(roll_mean_col)\n",
        "#     # Rolling std\n",
        "#     roll_std_col = f\"{col}_roll{window}_std\"\n",
        "#     train_df_clean[roll_std_col] = train_df_clean.groupby('asset_id')[col].rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "#     rolling_features.append(roll_std_col)\n",
        "\n",
        "# # Drop rows with NaN after lagging (optional, or impute later)\n",
        "# train_df_clean = train_df_clean.dropna(subset=lag_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "899c0baf",
      "metadata": {
        "id": "899c0baf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Aggregate features\n",
        "def aggregate_features(df, is_train=True):\n",
        "    agg_funcs = ['mean', 'std', 'min', 'max']\n",
        "    feature_cols = [col for col in df.columns if col.startswith(\"indicator_\")]\n",
        "\n",
        "    aggregated = df.groupby('asset_id')[feature_cols].agg(agg_funcs)\n",
        "    aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\n",
        "\n",
        "    if is_train:\n",
        "        static_cols = ['return_on_asset', 'company_age', 'company_size', 'revenue']\n",
        "    else:\n",
        "        static_cols = ['company_age', 'company_size', 'revenue']\n",
        "\n",
        "    static_info = df.groupby('asset_id')[static_cols].first()\n",
        "    return aggregated.join(static_info)\n",
        "\n",
        "train_agg = aggregate_features(train_df, is_train=True)\n",
        "test_agg = aggregate_features(test_df, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c89e92a4",
      "metadata": {
        "id": "c89e92a4"
      },
      "outputs": [],
      "source": [
        "# Step 2: Prepare training data\n",
        "X_train = train_agg.drop(columns=[\"return_on_asset\"])\n",
        "y_train = train_agg[\"return_on_asset\"]\n",
        "groups = train_agg.index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "22559447",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Step 3: Impute missing values using IterativeImputer\n",
        "imputer = IterativeImputer(random_state=42)\n",
        "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "test_agg_imputed = pd.DataFrame(imputer.transform(test_agg), index=test_agg.index, columns=test_agg.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fa8e1802",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = train_agg[\"return_on_asset\"]\n",
        "y_train = np.clip(y_train, a_min=-100, a_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "03aefd0a",
      "metadata": {
        "id": "03aefd0a",
        "outputId": "f62d8483-403b-4923-dd18-92a31bce2194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAE scores: [8.001262135922332, 7.372254901960783, 7.230686274509804, 6.949901960784314, 7.492450980392157]\n",
            "Average MAE: 7.409311250713879\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Cross-validated training\n",
        "\n",
        "# Ensure X_train_imputed is defined\n",
        "if 'X_train_imputed' not in globals():\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train = train_agg.drop(columns=[\"return_on_asset\"])\n",
        "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "\n",
        "cv = GroupKFold(n_splits=5)\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
        "cv_scores = []\n",
        "\n",
        "# Use 'groups' as defined in cell 4 (groups = train_agg.index)\n",
        "for train_idx, val_idx in cv.split(X_train_imputed, y_train, groups):\n",
        "    X_tr, X_val = X_train_imputed.iloc[train_idx], X_train_imputed.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "    preds = model.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, preds)\n",
        "    cv_scores.append(score)\n",
        "\n",
        "print(\"CV MAE scores:\", cv_scores)\n",
        "print(\"Average MAE:\", np.mean(cv_scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6bbd827b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Bagging: RandomForest (already used in your notebook)\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train_imputed, y_train)\n",
        "rf_preds = rf.predict(test_agg_imputed)\n",
        "\n",
        "# Boosting: GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
        "gbr.fit(X_train_imputed, y_train)\n",
        "gbr_preds = gbr.predict(test_agg_imputed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "73dae0be",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Prepare sequence data for each asset_id\n",
        "class AssetSequenceDataset(Dataset):\n",
        "    def __init__(self, df, target_col, seq_len=32, feature_cols=None):\n",
        "        self.groups = []\n",
        "        self.sequences = []\n",
        "        self.targets = []\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        if feature_cols is None:\n",
        "            feature_cols = [c for c in df.columns if c.startswith('indicator_')]\n",
        "        self.feature_cols = feature_cols\n",
        "\n",
        "        grouped = df.groupby('asset_id')\n",
        "        for asset_id, group in grouped:\n",
        "            group = group.sort_values('timestamp')\n",
        "            features = group[self.feature_cols].values\n",
        "            target = group[target_col].values if target_col in group else None\n",
        "            # Pad or truncate\n",
        "            if len(features) < seq_len:\n",
        "                pad_width = seq_len - len(features)\n",
        "                features = np.pad(features, ((pad_width,0),(0,0)), 'constant')\n",
        "                if target is not None:\n",
        "                    target = np.pad(target, (pad_width,0), 'constant')\n",
        "            else:\n",
        "                features = features[-seq_len:]\n",
        "                if target is not None:\n",
        "                    target = target[-seq_len:]\n",
        "            self.sequences.append(features)\n",
        "            if target is not None:\n",
        "                self.targets.append(target[-1])  # predict last value\n",
        "            self.groups.append(asset_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.targets[idx], dtype=torch.float32) if self.targets else torch.tensor(0.0)\n",
        "        return x, y\n",
        "\n",
        "# Temporal Transformer Model\n",
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, seq_len=32):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(d_model * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.transformer(x)\n",
        "        return self.head(x)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8cc0de3e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAE scores: [8.001262135922332, 7.372254901960783, 7.230686274509804, 6.949901960784314, 7.492450980392157]\n",
            "Average MAE: 7.409311250713879\n"
          ]
        }
      ],
      "source": [
        "print(\"CV MAE scores:\", cv_scores)\n",
        "print(\"Average MAE:\", np.mean(cv_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "20c15e64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average predictions from bagging and boosting models\n",
        "ensemble_preds = 0.5 * rf_preds + 0.5 * gbr_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1aca2e6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train MAE: 41.7667\n",
            "Epoch 2, Train MAE: 40.2615\n",
            "Epoch 3, Train MAE: 38.2489\n",
            "Epoch 4, Train MAE: 35.6049\n",
            "Epoch 5, Train MAE: 32.2204\n",
            "Epoch 6, Train MAE: 28.0245\n",
            "Epoch 7, Train MAE: 23.3403\n",
            "Epoch 8, Train MAE: 19.2658\n",
            "Epoch 9, Train MAE: 16.6440\n",
            "Epoch 10, Train MAE: 15.5928\n",
            "Epoch 11, Train MAE: 15.5521\n",
            "Epoch 12, Train MAE: 17.1206\n",
            "Epoch 13, Train MAE: 17.1303\n",
            "Epoch 14, Train MAE: 16.5302\n",
            "Epoch 15, Train MAE: 17.8587\n",
            "Epoch 16, Train MAE: 17.8014\n",
            "Epoch 17, Train MAE: 20.4252\n",
            "Epoch 18, Train MAE: 17.7143\n",
            "Epoch 19, Train MAE: 18.8478\n",
            "Epoch 20, Train MAE: 18.8537\n",
            "Epoch 21, Train MAE: 19.0086\n",
            "Epoch 22, Train MAE: 19.3162\n",
            "Epoch 23, Train MAE: 20.7671\n",
            "Epoch 24, Train MAE: 19.0804\n",
            "Epoch 25, Train MAE: 19.0590\n",
            "Epoch 26, Train MAE: 19.5410\n",
            "Epoch 27, Train MAE: 20.0977\n",
            "Epoch 28, Train MAE: 20.2943\n",
            "Epoch 29, Train MAE: 19.7373\n",
            "Epoch 30, Train MAE: 20.2071\n",
            "Epoch 31, Train MAE: 20.7259\n",
            "Epoch 32, Train MAE: 20.4242\n",
            "Epoch 33, Train MAE: 19.9785\n",
            "Epoch 34, Train MAE: 19.3259\n",
            "Epoch 35, Train MAE: 20.3209\n",
            "Epoch 36, Train MAE: 20.6046\n",
            "Epoch 37, Train MAE: 20.8231\n",
            "Epoch 38, Train MAE: 21.3449\n",
            "Epoch 39, Train MAE: 21.7029\n",
            "Epoch 40, Train MAE: 21.6632\n",
            "Epoch 41, Train MAE: 19.8879\n",
            "Epoch 42, Train MAE: 22.0847\n",
            "Epoch 43, Train MAE: 20.6039\n",
            "Epoch 44, Train MAE: 20.9053\n",
            "Epoch 45, Train MAE: 20.1317\n",
            "Epoch 46, Train MAE: 21.7938\n",
            "Epoch 47, Train MAE: 21.7485\n",
            "Epoch 48, Train MAE: 21.0627\n",
            "Epoch 49, Train MAE: 20.4384\n",
            "Epoch 50, Train MAE: 21.9349\n",
            "Epoch 51, Train MAE: 21.9200\n",
            "Epoch 52, Train MAE: 22.9287\n",
            "Epoch 53, Train MAE: 21.7200\n",
            "Epoch 54, Train MAE: 21.9036\n",
            "Epoch 55, Train MAE: 21.2683\n",
            "Epoch 56, Train MAE: 20.9112\n",
            "Epoch 57, Train MAE: 21.8147\n",
            "Epoch 58, Train MAE: 22.4471\n",
            "Epoch 59, Train MAE: 22.4301\n",
            "Epoch 60, Train MAE: 20.9832\n",
            "Epoch 61, Train MAE: 20.5138\n",
            "Epoch 62, Train MAE: 22.8987\n",
            "Epoch 63, Train MAE: 20.9599\n",
            "Epoch 64, Train MAE: 21.0817\n",
            "Epoch 65, Train MAE: 21.2505\n",
            "Epoch 66, Train MAE: 21.9882\n",
            "Epoch 67, Train MAE: 20.9983\n",
            "Epoch 68, Train MAE: 24.0273\n",
            "Epoch 69, Train MAE: 20.9978\n",
            "Epoch 70, Train MAE: 22.0364\n",
            "Epoch 71, Train MAE: 20.7367\n",
            "Epoch 72, Train MAE: 21.8972\n",
            "Epoch 73, Train MAE: 22.4910\n",
            "Epoch 74, Train MAE: 23.2538\n",
            "Epoch 75, Train MAE: 21.0167\n",
            "Epoch 76, Train MAE: 23.1474\n",
            "Epoch 77, Train MAE: 23.5425\n",
            "Epoch 78, Train MAE: 21.7119\n",
            "Epoch 79, Train MAE: 22.4099\n",
            "Epoch 80, Train MAE: 22.8158\n",
            "Epoch 81, Train MAE: 22.5061\n",
            "Epoch 82, Train MAE: 21.2543\n",
            "Epoch 83, Train MAE: 22.3338\n",
            "Epoch 84, Train MAE: 21.5485\n",
            "Epoch 85, Train MAE: 22.7917\n",
            "Epoch 86, Train MAE: 22.5088\n",
            "Epoch 87, Train MAE: 22.5001\n",
            "Epoch 88, Train MAE: 21.1339\n",
            "Epoch 89, Train MAE: 22.8851\n",
            "Epoch 90, Train MAE: 22.4480\n",
            "Epoch 91, Train MAE: 23.3990\n",
            "Epoch 92, Train MAE: 23.4472\n",
            "Epoch 93, Train MAE: 21.3973\n",
            "Epoch 94, Train MAE: 21.6782\n",
            "Epoch 95, Train MAE: 21.6823\n",
            "Epoch 96, Train MAE: 22.9959\n",
            "Epoch 97, Train MAE: 21.9355\n",
            "Epoch 98, Train MAE: 21.0591\n",
            "Epoch 99, Train MAE: 20.8134\n",
            "Epoch 100, Train MAE: 22.5240\n",
            "Epoch 101, Train MAE: 22.2026\n",
            "Epoch 102, Train MAE: 21.0813\n",
            "Epoch 103, Train MAE: 21.5707\n",
            "Epoch 104, Train MAE: 23.0838\n",
            "Epoch 105, Train MAE: 22.2582\n",
            "Epoch 106, Train MAE: 23.6624\n",
            "Epoch 107, Train MAE: 21.8889\n",
            "Epoch 108, Train MAE: 21.9794\n",
            "Epoch 109, Train MAE: 20.6576\n",
            "Epoch 110, Train MAE: 22.6366\n",
            "Epoch 111, Train MAE: 21.9364\n",
            "Epoch 112, Train MAE: 21.9947\n",
            "Epoch 113, Train MAE: 21.7310\n",
            "Epoch 114, Train MAE: 21.6621\n",
            "Epoch 115, Train MAE: 21.6196\n",
            "Epoch 116, Train MAE: 21.7033\n",
            "Epoch 117, Train MAE: 23.0144\n",
            "Epoch 118, Train MAE: 21.5859\n",
            "Epoch 119, Train MAE: 21.9277\n",
            "Epoch 120, Train MAE: 22.4399\n",
            "Epoch 121, Train MAE: 22.2068\n",
            "Epoch 122, Train MAE: 23.1096\n",
            "Epoch 123, Train MAE: 21.4758\n",
            "Epoch 124, Train MAE: 22.0384\n",
            "Epoch 125, Train MAE: 22.2943\n",
            "Epoch 126, Train MAE: 22.3722\n",
            "Epoch 127, Train MAE: 22.1798\n",
            "Epoch 128, Train MAE: 22.5266\n",
            "Epoch 129, Train MAE: 21.8808\n",
            "Epoch 130, Train MAE: 21.8019\n",
            "Epoch 131, Train MAE: 21.9401\n",
            "Epoch 132, Train MAE: 23.5277\n",
            "Epoch 133, Train MAE: 21.6175\n",
            "Epoch 134, Train MAE: 21.6715\n",
            "Epoch 135, Train MAE: 21.5024\n",
            "Epoch 136, Train MAE: 21.8360\n",
            "Epoch 137, Train MAE: 22.7689\n",
            "Epoch 138, Train MAE: 22.4518\n",
            "Epoch 139, Train MAE: 22.3917\n",
            "Epoch 140, Train MAE: 21.6462\n",
            "Epoch 141, Train MAE: 22.3081\n",
            "Epoch 142, Train MAE: 22.2824\n",
            "Epoch 143, Train MAE: 22.2088\n",
            "Epoch 144, Train MAE: 21.9260\n",
            "Epoch 145, Train MAE: 21.7526\n",
            "Epoch 146, Train MAE: 21.1477\n",
            "Epoch 147, Train MAE: 22.9476\n",
            "Epoch 148, Train MAE: 22.4351\n",
            "Epoch 149, Train MAE: 22.2229\n",
            "Epoch 150, Train MAE: 23.1219\n",
            "Epoch 151, Train MAE: 22.6600\n",
            "Epoch 152, Train MAE: 23.0313\n",
            "Epoch 153, Train MAE: 21.7168\n",
            "Epoch 154, Train MAE: 21.6363\n",
            "Epoch 155, Train MAE: 20.9708\n",
            "Epoch 156, Train MAE: 22.1682\n",
            "Epoch 157, Train MAE: 23.3788\n",
            "Epoch 158, Train MAE: 22.4335\n",
            "Epoch 159, Train MAE: 21.5486\n",
            "Epoch 160, Train MAE: 21.6610\n",
            "Epoch 161, Train MAE: 21.7228\n",
            "Epoch 162, Train MAE: 22.3461\n",
            "Epoch 163, Train MAE: 20.8712\n",
            "Epoch 164, Train MAE: 22.1048\n",
            "Epoch 165, Train MAE: 21.7089\n",
            "Epoch 166, Train MAE: 22.6915\n",
            "Epoch 167, Train MAE: 22.2551\n",
            "Epoch 168, Train MAE: 21.6694\n",
            "Epoch 169, Train MAE: 23.4313\n",
            "Epoch 170, Train MAE: 21.9469\n",
            "Epoch 171, Train MAE: 22.6287\n",
            "Epoch 172, Train MAE: 22.1848\n",
            "Epoch 173, Train MAE: 23.3169\n",
            "Epoch 174, Train MAE: 22.8583\n",
            "Epoch 175, Train MAE: 22.5115\n",
            "Epoch 176, Train MAE: 22.0116\n",
            "Epoch 177, Train MAE: 21.0743\n",
            "Epoch 178, Train MAE: 21.8931\n",
            "Epoch 179, Train MAE: 21.5353\n",
            "Epoch 180, Train MAE: 21.4738\n",
            "Epoch 181, Train MAE: 21.1177\n",
            "Epoch 182, Train MAE: 22.2979\n",
            "Epoch 183, Train MAE: 21.0472\n",
            "Epoch 184, Train MAE: 21.1491\n",
            "Epoch 185, Train MAE: 21.7966\n",
            "Epoch 186, Train MAE: 20.6752\n",
            "Epoch 187, Train MAE: 21.8599\n",
            "Epoch 188, Train MAE: 20.5506\n",
            "Epoch 189, Train MAE: 22.3066\n",
            "Epoch 190, Train MAE: 22.4056\n",
            "Epoch 191, Train MAE: 22.4753\n",
            "Epoch 192, Train MAE: 22.8531\n",
            "Epoch 193, Train MAE: 22.9134\n",
            "Epoch 194, Train MAE: 21.8813\n",
            "Epoch 195, Train MAE: 21.6268\n",
            "Epoch 196, Train MAE: 21.1576\n",
            "Epoch 197, Train MAE: 21.7710\n",
            "Epoch 198, Train MAE: 23.2231\n",
            "Epoch 199, Train MAE: 23.0814\n",
            "Epoch 200, Train MAE: 21.5751\n",
            "Epoch 201, Train MAE: 22.6659\n",
            "Epoch 202, Train MAE: 22.2195\n",
            "Epoch 203, Train MAE: 21.9716\n",
            "Epoch 204, Train MAE: 22.1410\n",
            "Epoch 205, Train MAE: 21.8080\n",
            "Epoch 206, Train MAE: 22.0473\n",
            "Epoch 207, Train MAE: 21.9288\n",
            "Epoch 208, Train MAE: 21.8670\n",
            "Epoch 209, Train MAE: 21.8400\n",
            "Epoch 210, Train MAE: 21.6883\n",
            "Epoch 211, Train MAE: 23.1094\n",
            "Epoch 212, Train MAE: 22.3583\n",
            "Epoch 213, Train MAE: 21.3195\n",
            "Epoch 214, Train MAE: 22.3982\n",
            "Epoch 215, Train MAE: 22.9737\n",
            "Epoch 216, Train MAE: 22.3943\n",
            "Epoch 217, Train MAE: 22.0864\n",
            "Epoch 218, Train MAE: 22.7506\n",
            "Epoch 219, Train MAE: 22.4968\n",
            "Epoch 220, Train MAE: 21.8125\n",
            "Epoch 221, Train MAE: 23.4319\n",
            "Epoch 222, Train MAE: 22.5472\n",
            "Epoch 223, Train MAE: 22.2390\n",
            "Epoch 224, Train MAE: 21.9027\n",
            "Epoch 225, Train MAE: 21.3870\n",
            "Epoch 226, Train MAE: 21.2282\n",
            "Epoch 227, Train MAE: 22.7056\n",
            "Epoch 228, Train MAE: 22.4554\n",
            "Epoch 229, Train MAE: 22.0427\n",
            "Epoch 230, Train MAE: 22.2842\n",
            "Epoch 231, Train MAE: 21.8619\n",
            "Epoch 232, Train MAE: 22.7396\n",
            "Epoch 233, Train MAE: 22.3013\n",
            "Epoch 234, Train MAE: 21.8641\n",
            "Epoch 235, Train MAE: 21.8942\n",
            "Epoch 236, Train MAE: 22.2231\n",
            "Epoch 237, Train MAE: 23.1734\n",
            "Epoch 238, Train MAE: 22.4274\n",
            "Epoch 239, Train MAE: 21.6492\n",
            "Epoch 240, Train MAE: 21.6657\n",
            "Epoch 241, Train MAE: 22.8918\n",
            "Epoch 242, Train MAE: 22.5078\n",
            "Epoch 243, Train MAE: 21.5982\n",
            "Epoch 244, Train MAE: 22.4799\n",
            "Epoch 245, Train MAE: 21.8049\n",
            "Epoch 246, Train MAE: 21.9649\n",
            "Epoch 247, Train MAE: 22.3343\n",
            "Epoch 248, Train MAE: 22.9860\n",
            "Epoch 249, Train MAE: 21.7875\n",
            "Epoch 250, Train MAE: 21.9150\n",
            "Epoch 251, Train MAE: 22.1657\n",
            "Epoch 252, Train MAE: 22.4345\n",
            "Epoch 253, Train MAE: 22.6325\n",
            "Epoch 254, Train MAE: 21.9275\n",
            "Epoch 255, Train MAE: 22.1805\n",
            "Epoch 256, Train MAE: 22.3005\n",
            "Epoch 257, Train MAE: 21.8427\n",
            "Epoch 258, Train MAE: 22.7474\n",
            "Epoch 259, Train MAE: 21.9890\n",
            "Epoch 260, Train MAE: 21.7988\n",
            "Epoch 261, Train MAE: 23.0487\n",
            "Epoch 262, Train MAE: 22.3242\n",
            "Epoch 263, Train MAE: 22.4885\n",
            "Epoch 264, Train MAE: 22.4443\n",
            "Epoch 265, Train MAE: 22.3265\n",
            "Epoch 266, Train MAE: 22.0682\n",
            "Epoch 267, Train MAE: 22.7509\n",
            "Epoch 268, Train MAE: 22.1733\n",
            "Epoch 269, Train MAE: 21.4385\n",
            "Epoch 270, Train MAE: 21.7090\n",
            "Epoch 271, Train MAE: 21.1420\n",
            "Epoch 272, Train MAE: 21.9959\n",
            "Epoch 273, Train MAE: 22.0405\n",
            "Epoch 274, Train MAE: 21.8605\n",
            "Epoch 275, Train MAE: 21.7243\n",
            "Epoch 276, Train MAE: 21.9602\n",
            "Epoch 277, Train MAE: 22.8211\n",
            "Epoch 278, Train MAE: 22.0907\n",
            "Epoch 279, Train MAE: 22.7673\n",
            "Epoch 280, Train MAE: 23.5651\n",
            "Epoch 281, Train MAE: 22.3858\n",
            "Epoch 282, Train MAE: 22.1040\n",
            "Epoch 283, Train MAE: 22.4751\n",
            "Epoch 284, Train MAE: 21.7058\n",
            "Epoch 285, Train MAE: 22.6070\n",
            "Epoch 286, Train MAE: 21.9315\n",
            "Epoch 287, Train MAE: 21.8586\n",
            "Epoch 288, Train MAE: 21.8873\n",
            "Epoch 289, Train MAE: 21.5008\n",
            "Epoch 290, Train MAE: 21.9236\n",
            "Epoch 291, Train MAE: 21.2349\n",
            "Epoch 292, Train MAE: 22.0255\n",
            "Epoch 293, Train MAE: 22.0407\n",
            "Epoch 294, Train MAE: 21.8973\n",
            "Epoch 295, Train MAE: 22.0526\n",
            "Epoch 296, Train MAE: 22.9293\n",
            "Epoch 297, Train MAE: 22.9236\n",
            "Epoch 298, Train MAE: 22.0510\n",
            "Epoch 299, Train MAE: 22.4732\n",
            "Epoch 300, Train MAE: 22.3433\n",
            "Epoch 301, Train MAE: 23.0193\n",
            "Epoch 302, Train MAE: 21.6377\n",
            "Epoch 303, Train MAE: 22.1755\n",
            "Epoch 304, Train MAE: 21.9630\n",
            "Epoch 305, Train MAE: 21.5089\n",
            "Epoch 306, Train MAE: 21.2960\n",
            "Epoch 307, Train MAE: 21.9570\n",
            "Epoch 308, Train MAE: 22.3788\n",
            "Epoch 309, Train MAE: 22.4387\n",
            "Epoch 310, Train MAE: 21.8316\n",
            "Epoch 311, Train MAE: 22.0441\n",
            "Epoch 312, Train MAE: 22.2959\n",
            "Epoch 313, Train MAE: 21.9371\n",
            "Epoch 314, Train MAE: 21.4285\n",
            "Epoch 315, Train MAE: 22.5852\n",
            "Epoch 316, Train MAE: 22.2948\n",
            "Epoch 317, Train MAE: 21.7669\n",
            "Epoch 318, Train MAE: 22.0265\n",
            "Epoch 319, Train MAE: 22.0630\n",
            "Epoch 320, Train MAE: 22.9745\n",
            "Epoch 321, Train MAE: 21.9543\n",
            "Epoch 322, Train MAE: 22.6070\n",
            "Epoch 323, Train MAE: 21.9704\n",
            "Epoch 324, Train MAE: 22.6876\n",
            "Epoch 325, Train MAE: 22.6336\n",
            "Epoch 326, Train MAE: 21.4909\n",
            "Epoch 327, Train MAE: 22.7062\n",
            "Epoch 328, Train MAE: 22.9979\n",
            "Epoch 329, Train MAE: 21.7369\n",
            "Epoch 330, Train MAE: 22.3212\n",
            "Epoch 331, Train MAE: 21.3406\n",
            "Epoch 332, Train MAE: 22.0202\n",
            "Epoch 333, Train MAE: 21.2730\n",
            "Epoch 334, Train MAE: 21.6971\n",
            "Epoch 335, Train MAE: 21.5933\n",
            "Epoch 336, Train MAE: 20.4108\n",
            "Epoch 337, Train MAE: 22.2491\n",
            "Epoch 338, Train MAE: 22.7053\n",
            "Epoch 339, Train MAE: 21.6032\n",
            "Epoch 340, Train MAE: 22.0961\n",
            "Epoch 341, Train MAE: 21.9791\n",
            "Epoch 342, Train MAE: 22.2409\n",
            "Epoch 343, Train MAE: 22.3804\n",
            "Epoch 344, Train MAE: 23.4064\n",
            "Epoch 345, Train MAE: 22.4388\n",
            "Epoch 346, Train MAE: 23.4423\n",
            "Epoch 347, Train MAE: 22.3281\n",
            "Epoch 348, Train MAE: 22.2709\n",
            "Epoch 349, Train MAE: 21.5415\n",
            "Epoch 350, Train MAE: 22.0422\n",
            "Epoch 351, Train MAE: 21.9569\n",
            "Epoch 352, Train MAE: 22.3768\n",
            "Epoch 353, Train MAE: 21.7206\n",
            "Epoch 354, Train MAE: 24.5518\n",
            "Epoch 355, Train MAE: 21.9117\n",
            "Epoch 356, Train MAE: 23.5322\n",
            "Epoch 357, Train MAE: 20.7576\n",
            "Epoch 358, Train MAE: 22.0262\n",
            "Epoch 359, Train MAE: 21.7537\n",
            "Epoch 360, Train MAE: 22.9753\n",
            "Epoch 361, Train MAE: 21.8172\n",
            "Epoch 362, Train MAE: 21.8147\n",
            "Epoch 363, Train MAE: 22.2869\n",
            "Epoch 364, Train MAE: 23.0082\n",
            "Epoch 365, Train MAE: 21.9186\n",
            "Epoch 366, Train MAE: 22.2771\n",
            "Epoch 367, Train MAE: 22.7414\n",
            "Epoch 368, Train MAE: 21.7779\n",
            "Epoch 369, Train MAE: 22.5714\n",
            "Epoch 370, Train MAE: 22.5908\n",
            "Epoch 371, Train MAE: 21.6127\n",
            "Epoch 372, Train MAE: 23.4193\n",
            "Epoch 373, Train MAE: 22.5839\n",
            "Epoch 374, Train MAE: 20.8221\n",
            "Epoch 375, Train MAE: 22.2764\n",
            "Epoch 376, Train MAE: 21.3497\n",
            "Epoch 377, Train MAE: 22.4601\n",
            "Epoch 378, Train MAE: 21.2422\n",
            "Epoch 379, Train MAE: 21.4702\n",
            "Epoch 380, Train MAE: 22.5536\n",
            "Epoch 381, Train MAE: 22.2280\n",
            "Epoch 382, Train MAE: 22.5984\n",
            "Epoch 383, Train MAE: 22.0891\n",
            "Epoch 384, Train MAE: 22.2466\n",
            "Epoch 385, Train MAE: 21.6757\n",
            "Epoch 386, Train MAE: 21.8182\n",
            "Epoch 387, Train MAE: 22.4994\n",
            "Epoch 388, Train MAE: 21.5580\n",
            "Epoch 389, Train MAE: 21.6734\n",
            "Epoch 390, Train MAE: 22.2748\n",
            "Epoch 391, Train MAE: 22.2183\n",
            "Epoch 392, Train MAE: 22.3966\n",
            "Epoch 393, Train MAE: 21.9371\n",
            "Epoch 394, Train MAE: 21.5653\n",
            "Epoch 395, Train MAE: 22.3810\n",
            "Epoch 396, Train MAE: 21.2876\n",
            "Epoch 397, Train MAE: 23.5635\n",
            "Epoch 398, Train MAE: 22.3897\n",
            "Epoch 399, Train MAE: 21.3724\n",
            "Epoch 400, Train MAE: 21.3838\n",
            "Epoch 401, Train MAE: 22.0848\n",
            "Epoch 402, Train MAE: 21.5323\n",
            "Epoch 403, Train MAE: 22.0705\n",
            "Epoch 404, Train MAE: 22.8768\n",
            "Epoch 405, Train MAE: 21.5922\n",
            "Epoch 406, Train MAE: 22.2932\n",
            "Epoch 407, Train MAE: 22.1308\n",
            "Epoch 408, Train MAE: 23.0031\n",
            "Epoch 409, Train MAE: 22.5717\n",
            "Epoch 410, Train MAE: 22.6349\n",
            "Epoch 411, Train MAE: 21.2941\n",
            "Epoch 412, Train MAE: 22.8781\n",
            "Epoch 413, Train MAE: 21.7600\n",
            "Epoch 414, Train MAE: 22.4459\n",
            "Epoch 415, Train MAE: 23.0904\n",
            "Epoch 416, Train MAE: 22.3866\n",
            "Epoch 417, Train MAE: 22.1726\n",
            "Epoch 418, Train MAE: 22.3959\n",
            "Epoch 419, Train MAE: 22.6158\n",
            "Epoch 420, Train MAE: 22.7180\n",
            "Epoch 421, Train MAE: 22.6295\n",
            "Epoch 422, Train MAE: 21.9636\n",
            "Epoch 423, Train MAE: 22.3296\n",
            "Epoch 424, Train MAE: 21.9601\n",
            "Epoch 425, Train MAE: 22.2694\n",
            "Epoch 426, Train MAE: 21.4539\n",
            "Epoch 427, Train MAE: 21.2038\n",
            "Epoch 428, Train MAE: 21.9534\n",
            "Epoch 429, Train MAE: 22.1833\n",
            "Epoch 430, Train MAE: 22.5143\n",
            "Epoch 431, Train MAE: 21.7553\n",
            "Epoch 432, Train MAE: 21.0360\n",
            "Epoch 433, Train MAE: 21.8318\n",
            "Epoch 434, Train MAE: 21.9877\n",
            "Epoch 435, Train MAE: 21.7627\n",
            "Epoch 436, Train MAE: 21.2348\n",
            "Epoch 437, Train MAE: 22.2140\n",
            "Epoch 438, Train MAE: 21.8423\n",
            "Epoch 439, Train MAE: 23.1338\n",
            "Epoch 440, Train MAE: 22.3815\n",
            "Epoch 441, Train MAE: 22.6455\n",
            "Epoch 442, Train MAE: 22.6677\n",
            "Epoch 443, Train MAE: 22.4753\n",
            "Epoch 444, Train MAE: 22.8612\n",
            "Epoch 445, Train MAE: 21.2461\n",
            "Epoch 446, Train MAE: 22.3523\n",
            "Epoch 447, Train MAE: 21.7781\n",
            "Epoch 448, Train MAE: 21.6130\n",
            "Epoch 449, Train MAE: 23.1333\n",
            "Epoch 450, Train MAE: 22.9245\n",
            "Epoch 451, Train MAE: 22.3319\n",
            "Epoch 452, Train MAE: 21.8923\n",
            "Epoch 453, Train MAE: 22.6460\n",
            "Epoch 454, Train MAE: 22.4736\n",
            "Epoch 455, Train MAE: 21.3265\n",
            "Epoch 456, Train MAE: 22.3335\n",
            "Epoch 457, Train MAE: 21.6637\n",
            "Epoch 458, Train MAE: 22.4492\n",
            "Epoch 459, Train MAE: 23.1049\n",
            "Epoch 460, Train MAE: 21.9276\n",
            "Epoch 461, Train MAE: 21.2359\n",
            "Epoch 462, Train MAE: 22.1806\n",
            "Epoch 463, Train MAE: 21.6556\n",
            "Epoch 464, Train MAE: 21.8557\n",
            "Epoch 465, Train MAE: 21.3337\n",
            "Epoch 466, Train MAE: 22.0480\n",
            "Epoch 467, Train MAE: 21.9557\n",
            "Epoch 468, Train MAE: 22.3633\n",
            "Epoch 469, Train MAE: 21.9713\n",
            "Epoch 470, Train MAE: 21.3171\n",
            "Epoch 471, Train MAE: 21.9980\n",
            "Epoch 472, Train MAE: 22.4086\n",
            "Epoch 473, Train MAE: 21.6084\n",
            "Epoch 474, Train MAE: 24.0072\n",
            "Epoch 475, Train MAE: 20.5712\n",
            "Epoch 476, Train MAE: 22.1531\n",
            "Epoch 477, Train MAE: 22.7126\n",
            "Epoch 478, Train MAE: 21.3506\n",
            "Epoch 479, Train MAE: 21.2327\n",
            "Epoch 480, Train MAE: 21.3164\n",
            "Epoch 481, Train MAE: 22.0501\n",
            "Epoch 482, Train MAE: 21.9452\n",
            "Epoch 483, Train MAE: 21.2530\n",
            "Epoch 484, Train MAE: 20.5218\n",
            "Epoch 485, Train MAE: 22.7314\n",
            "Epoch 486, Train MAE: 23.1862\n",
            "Epoch 487, Train MAE: 22.5470\n",
            "Epoch 488, Train MAE: 20.4474\n",
            "Epoch 489, Train MAE: 21.6797\n",
            "Epoch 490, Train MAE: 22.6725\n",
            "Epoch 491, Train MAE: 23.7410\n",
            "Epoch 492, Train MAE: 21.4863\n",
            "Epoch 493, Train MAE: 21.4485\n",
            "Epoch 494, Train MAE: 21.5997\n",
            "Epoch 495, Train MAE: 23.0947\n",
            "Epoch 496, Train MAE: 22.2005\n",
            "Epoch 497, Train MAE: 22.7203\n",
            "Epoch 498, Train MAE: 22.7923\n",
            "Epoch 499, Train MAE: 22.5628\n",
            "Epoch 500, Train MAE: 20.8560\n",
            "Epoch 501, Train MAE: 21.6241\n",
            "Epoch 502, Train MAE: 21.9382\n",
            "Epoch 503, Train MAE: 22.6063\n",
            "Epoch 504, Train MAE: 22.4234\n",
            "Epoch 505, Train MAE: 20.7623\n",
            "Epoch 506, Train MAE: 22.4157\n",
            "Epoch 507, Train MAE: 21.9422\n",
            "Epoch 508, Train MAE: 21.1986\n",
            "Epoch 509, Train MAE: 21.9512\n",
            "Epoch 510, Train MAE: 22.9718\n",
            "Epoch 511, Train MAE: 22.6770\n",
            "Epoch 512, Train MAE: 21.4806\n",
            "Epoch 513, Train MAE: 21.5605\n",
            "Epoch 514, Train MAE: 21.4066\n",
            "Epoch 515, Train MAE: 22.5985\n",
            "Epoch 516, Train MAE: 21.7430\n",
            "Epoch 517, Train MAE: 20.9973\n",
            "Epoch 518, Train MAE: 21.8594\n",
            "Epoch 519, Train MAE: 23.2283\n",
            "Epoch 520, Train MAE: 22.0636\n",
            "Epoch 521, Train MAE: 22.5460\n",
            "Epoch 522, Train MAE: 21.5865\n",
            "Epoch 523, Train MAE: 22.3095\n",
            "Epoch 524, Train MAE: 22.6861\n",
            "Epoch 525, Train MAE: 21.1259\n",
            "Epoch 526, Train MAE: 22.1660\n",
            "Epoch 527, Train MAE: 22.5095\n",
            "Epoch 528, Train MAE: 21.1457\n",
            "Epoch 529, Train MAE: 21.8029\n",
            "Epoch 530, Train MAE: 22.7009\n",
            "Epoch 531, Train MAE: 20.9298\n",
            "Epoch 532, Train MAE: 21.6212\n",
            "Epoch 533, Train MAE: 23.5006\n",
            "Epoch 534, Train MAE: 22.4335\n",
            "Epoch 535, Train MAE: 22.7686\n",
            "Epoch 536, Train MAE: 21.9648\n",
            "Epoch 537, Train MAE: 21.7907\n",
            "Epoch 538, Train MAE: 21.2411\n",
            "Epoch 539, Train MAE: 21.7317\n",
            "Epoch 540, Train MAE: 21.7878\n",
            "Epoch 541, Train MAE: 22.2342\n",
            "Epoch 542, Train MAE: 21.4507\n",
            "Epoch 543, Train MAE: 21.1962\n",
            "Epoch 544, Train MAE: 22.0201\n",
            "Epoch 545, Train MAE: 22.0002\n",
            "Epoch 546, Train MAE: 22.0854\n",
            "Epoch 547, Train MAE: 22.2741\n",
            "Epoch 548, Train MAE: 21.5385\n",
            "Epoch 549, Train MAE: 22.2593\n",
            "Epoch 550, Train MAE: 22.4090\n",
            "Epoch 551, Train MAE: 21.5611\n",
            "Epoch 552, Train MAE: 21.7404\n",
            "Epoch 553, Train MAE: 22.4422\n",
            "Epoch 554, Train MAE: 21.1957\n",
            "Epoch 555, Train MAE: 22.0612\n",
            "Epoch 556, Train MAE: 21.7486\n",
            "Epoch 557, Train MAE: 22.4561\n",
            "Epoch 558, Train MAE: 23.3130\n",
            "Epoch 559, Train MAE: 22.4993\n",
            "Epoch 560, Train MAE: 21.6649\n",
            "Epoch 561, Train MAE: 22.4362\n",
            "Epoch 562, Train MAE: 22.7199\n",
            "Epoch 563, Train MAE: 23.1965\n",
            "Epoch 564, Train MAE: 22.0816\n",
            "Epoch 565, Train MAE: 22.4369\n",
            "Epoch 566, Train MAE: 21.8030\n",
            "Epoch 567, Train MAE: 22.1720\n",
            "Epoch 568, Train MAE: 22.7610\n",
            "Epoch 569, Train MAE: 22.3160\n",
            "Epoch 570, Train MAE: 22.2374\n",
            "Epoch 571, Train MAE: 21.5227\n",
            "Epoch 572, Train MAE: 22.7779\n",
            "Epoch 573, Train MAE: 22.2970\n",
            "Epoch 574, Train MAE: 22.1898\n",
            "Epoch 575, Train MAE: 21.8820\n",
            "Epoch 576, Train MAE: 21.9160\n",
            "Epoch 577, Train MAE: 21.6371\n",
            "Epoch 578, Train MAE: 21.9924\n",
            "Epoch 579, Train MAE: 21.5829\n",
            "Epoch 580, Train MAE: 22.6214\n",
            "Epoch 581, Train MAE: 22.5643\n",
            "Epoch 582, Train MAE: 21.7637\n",
            "Epoch 583, Train MAE: 21.3921\n",
            "Epoch 584, Train MAE: 22.0485\n",
            "Epoch 585, Train MAE: 22.7634\n",
            "Epoch 586, Train MAE: 22.0248\n",
            "Epoch 587, Train MAE: 22.1745\n",
            "Epoch 588, Train MAE: 21.4407\n",
            "Epoch 589, Train MAE: 22.7972\n",
            "Epoch 590, Train MAE: 21.8495\n",
            "Epoch 591, Train MAE: 21.5918\n",
            "Epoch 592, Train MAE: 22.5080\n",
            "Epoch 593, Train MAE: 21.7944\n",
            "Epoch 594, Train MAE: 22.1603\n",
            "Epoch 595, Train MAE: 22.0378\n",
            "Epoch 596, Train MAE: 21.1680\n",
            "Epoch 597, Train MAE: 23.3493\n",
            "Epoch 598, Train MAE: 21.4997\n",
            "Epoch 599, Train MAE: 22.2260\n",
            "Epoch 600, Train MAE: 22.3662\n",
            "Epoch 601, Train MAE: 22.1688\n",
            "Epoch 602, Train MAE: 22.4294\n",
            "Epoch 603, Train MAE: 21.6307\n",
            "Epoch 604, Train MAE: 21.9637\n",
            "Epoch 605, Train MAE: 21.1431\n",
            "Epoch 606, Train MAE: 22.1722\n",
            "Epoch 607, Train MAE: 22.2205\n",
            "Epoch 608, Train MAE: 22.2457\n",
            "Epoch 609, Train MAE: 22.9262\n",
            "Epoch 610, Train MAE: 22.1029\n",
            "Epoch 611, Train MAE: 22.9591\n",
            "Epoch 612, Train MAE: 21.3496\n",
            "Epoch 613, Train MAE: 22.5521\n",
            "Epoch 614, Train MAE: 22.9118\n",
            "Epoch 615, Train MAE: 21.7990\n",
            "Epoch 616, Train MAE: 23.2815\n",
            "Epoch 617, Train MAE: 22.4265\n",
            "Epoch 618, Train MAE: 22.1430\n",
            "Epoch 619, Train MAE: 22.4111\n",
            "Epoch 620, Train MAE: 22.5659\n",
            "Epoch 621, Train MAE: 22.1292\n",
            "Epoch 622, Train MAE: 23.0030\n",
            "Epoch 623, Train MAE: 22.4382\n",
            "Epoch 624, Train MAE: 21.7588\n",
            "Epoch 625, Train MAE: 21.3347\n",
            "Epoch 626, Train MAE: 22.1542\n",
            "Epoch 627, Train MAE: 22.6847\n",
            "Epoch 628, Train MAE: 21.1537\n",
            "Epoch 629, Train MAE: 21.9729\n",
            "Epoch 630, Train MAE: 22.5629\n",
            "Epoch 631, Train MAE: 20.9915\n",
            "Epoch 632, Train MAE: 22.2971\n",
            "Epoch 633, Train MAE: 22.3644\n",
            "Epoch 634, Train MAE: 22.2625\n",
            "Epoch 635, Train MAE: 21.8681\n",
            "Epoch 636, Train MAE: 22.3872\n",
            "Epoch 637, Train MAE: 21.8209\n",
            "Epoch 638, Train MAE: 22.2432\n",
            "Epoch 639, Train MAE: 20.8655\n",
            "Epoch 640, Train MAE: 22.2778\n",
            "Epoch 641, Train MAE: 22.7885\n",
            "Epoch 642, Train MAE: 22.8274\n",
            "Epoch 643, Train MAE: 23.1178\n",
            "Epoch 644, Train MAE: 22.7710\n",
            "Epoch 645, Train MAE: 21.2150\n",
            "Epoch 646, Train MAE: 22.0660\n",
            "Epoch 647, Train MAE: 21.5896\n",
            "Epoch 648, Train MAE: 22.6194\n",
            "Epoch 649, Train MAE: 21.9563\n",
            "Epoch 650, Train MAE: 21.5956\n",
            "Epoch 651, Train MAE: 21.1988\n",
            "Epoch 652, Train MAE: 22.5435\n",
            "Epoch 653, Train MAE: 22.6159\n",
            "Epoch 654, Train MAE: 21.3188\n",
            "Epoch 655, Train MAE: 23.1673\n",
            "Epoch 656, Train MAE: 20.5975\n",
            "Epoch 657, Train MAE: 21.7090\n",
            "Epoch 658, Train MAE: 22.2683\n",
            "Epoch 659, Train MAE: 22.3643\n",
            "Epoch 660, Train MAE: 21.4963\n",
            "Epoch 661, Train MAE: 21.2571\n",
            "Epoch 662, Train MAE: 21.3530\n",
            "Epoch 663, Train MAE: 21.7231\n",
            "Epoch 664, Train MAE: 21.3070\n",
            "Epoch 665, Train MAE: 22.9937\n",
            "Epoch 666, Train MAE: 21.7603\n",
            "Epoch 667, Train MAE: 22.5232\n",
            "Epoch 668, Train MAE: 22.4459\n",
            "Epoch 669, Train MAE: 22.4622\n",
            "Epoch 670, Train MAE: 21.9019\n",
            "Epoch 671, Train MAE: 20.9971\n",
            "Epoch 672, Train MAE: 22.4457\n",
            "Epoch 673, Train MAE: 21.8584\n",
            "Epoch 674, Train MAE: 22.7656\n",
            "Epoch 675, Train MAE: 22.8396\n",
            "Epoch 676, Train MAE: 21.0809\n",
            "Epoch 677, Train MAE: 21.6594\n",
            "Epoch 678, Train MAE: 22.4076\n",
            "Epoch 679, Train MAE: 22.2002\n",
            "Epoch 680, Train MAE: 22.4873\n",
            "Epoch 681, Train MAE: 22.6476\n",
            "Epoch 682, Train MAE: 22.0277\n",
            "Epoch 683, Train MAE: 21.7978\n",
            "Epoch 684, Train MAE: 21.7280\n",
            "Epoch 685, Train MAE: 21.6966\n",
            "Epoch 686, Train MAE: 22.5388\n",
            "Epoch 687, Train MAE: 22.4171\n",
            "Epoch 688, Train MAE: 22.6161\n",
            "Epoch 689, Train MAE: 21.7447\n",
            "Epoch 690, Train MAE: 21.6077\n",
            "Epoch 691, Train MAE: 22.3067\n",
            "Epoch 692, Train MAE: 22.2279\n",
            "Epoch 693, Train MAE: 22.2804\n",
            "Epoch 694, Train MAE: 21.7675\n",
            "Epoch 695, Train MAE: 22.7070\n",
            "Epoch 696, Train MAE: 22.1694\n",
            "Epoch 697, Train MAE: 22.1199\n",
            "Epoch 698, Train MAE: 21.8972\n",
            "Epoch 699, Train MAE: 20.4892\n",
            "Epoch 700, Train MAE: 22.3344\n",
            "Epoch 701, Train MAE: 22.0536\n",
            "Epoch 702, Train MAE: 22.8755\n",
            "Epoch 703, Train MAE: 22.8052\n",
            "Epoch 704, Train MAE: 21.0841\n",
            "Epoch 705, Train MAE: 22.2307\n",
            "Epoch 706, Train MAE: 22.6898\n",
            "Epoch 707, Train MAE: 21.6064\n",
            "Epoch 708, Train MAE: 21.3450\n",
            "Epoch 709, Train MAE: 20.9738\n",
            "Epoch 710, Train MAE: 21.6876\n",
            "Epoch 711, Train MAE: 21.9860\n",
            "Epoch 712, Train MAE: 21.2266\n",
            "Epoch 713, Train MAE: 22.0370\n",
            "Epoch 714, Train MAE: 21.5730\n",
            "Epoch 715, Train MAE: 22.2540\n",
            "Epoch 716, Train MAE: 22.1250\n",
            "Epoch 717, Train MAE: 21.8492\n",
            "Epoch 718, Train MAE: 20.7321\n",
            "Epoch 719, Train MAE: 21.8400\n",
            "Epoch 720, Train MAE: 23.0159\n",
            "Epoch 721, Train MAE: 23.3257\n",
            "Epoch 722, Train MAE: 21.2435\n",
            "Epoch 723, Train MAE: 21.7978\n",
            "Epoch 724, Train MAE: 21.8546\n",
            "Epoch 725, Train MAE: 22.5628\n",
            "Epoch 726, Train MAE: 22.4392\n",
            "Epoch 727, Train MAE: 22.5612\n",
            "Epoch 728, Train MAE: 21.8496\n",
            "Epoch 729, Train MAE: 21.9279\n",
            "Epoch 730, Train MAE: 23.1110\n",
            "Epoch 731, Train MAE: 21.7241\n",
            "Epoch 732, Train MAE: 23.1288\n",
            "Epoch 733, Train MAE: 22.3125\n",
            "Epoch 734, Train MAE: 21.7662\n",
            "Epoch 735, Train MAE: 21.8892\n",
            "Epoch 736, Train MAE: 22.8389\n",
            "Epoch 737, Train MAE: 22.5716\n",
            "Epoch 738, Train MAE: 21.3080\n",
            "Epoch 739, Train MAE: 22.5117\n",
            "Epoch 740, Train MAE: 21.7686\n",
            "Epoch 741, Train MAE: 22.7783\n",
            "Epoch 742, Train MAE: 21.0540\n",
            "Epoch 743, Train MAE: 22.5124\n",
            "Epoch 744, Train MAE: 22.9027\n",
            "Epoch 745, Train MAE: 22.5123\n",
            "Epoch 746, Train MAE: 22.2129\n",
            "Epoch 747, Train MAE: 22.5528\n",
            "Epoch 748, Train MAE: 21.4209\n",
            "Epoch 749, Train MAE: 21.0955\n",
            "Epoch 750, Train MAE: 23.1171\n",
            "Epoch 751, Train MAE: 22.1684\n",
            "Epoch 752, Train MAE: 22.0115\n",
            "Epoch 753, Train MAE: 21.9272\n",
            "Epoch 754, Train MAE: 22.2853\n",
            "Epoch 755, Train MAE: 21.5020\n",
            "Epoch 756, Train MAE: 21.1494\n",
            "Epoch 757, Train MAE: 21.7101\n",
            "Epoch 758, Train MAE: 22.0055\n",
            "Epoch 759, Train MAE: 22.2313\n",
            "Epoch 760, Train MAE: 21.6601\n",
            "Epoch 761, Train MAE: 22.3381\n",
            "Epoch 762, Train MAE: 21.9523\n",
            "Epoch 763, Train MAE: 23.5814\n",
            "Epoch 764, Train MAE: 22.0656\n",
            "Epoch 765, Train MAE: 22.2849\n",
            "Epoch 766, Train MAE: 21.8260\n",
            "Epoch 767, Train MAE: 21.8272\n",
            "Epoch 768, Train MAE: 22.8376\n",
            "Epoch 769, Train MAE: 20.8888\n",
            "Epoch 770, Train MAE: 21.3818\n",
            "Epoch 771, Train MAE: 21.6285\n",
            "Epoch 772, Train MAE: 21.8005\n",
            "Epoch 773, Train MAE: 22.1517\n",
            "Epoch 774, Train MAE: 22.1900\n",
            "Epoch 775, Train MAE: 22.6742\n",
            "Epoch 776, Train MAE: 22.2979\n",
            "Epoch 777, Train MAE: 21.2745\n",
            "Epoch 778, Train MAE: 22.9162\n",
            "Epoch 779, Train MAE: 23.0186\n",
            "Epoch 780, Train MAE: 21.9162\n",
            "Epoch 781, Train MAE: 22.4560\n",
            "Epoch 782, Train MAE: 22.2436\n",
            "Epoch 783, Train MAE: 22.8923\n",
            "Epoch 784, Train MAE: 21.5809\n",
            "Epoch 785, Train MAE: 22.6396\n",
            "Epoch 786, Train MAE: 21.5743\n",
            "Epoch 787, Train MAE: 21.7477\n",
            "Epoch 788, Train MAE: 21.5528\n",
            "Epoch 789, Train MAE: 21.8941\n",
            "Epoch 790, Train MAE: 21.9670\n",
            "Epoch 791, Train MAE: 21.8625\n",
            "Epoch 792, Train MAE: 21.0384\n",
            "Epoch 793, Train MAE: 22.8375\n",
            "Epoch 794, Train MAE: 23.0062\n",
            "Epoch 795, Train MAE: 22.6680\n",
            "Epoch 796, Train MAE: 21.7941\n",
            "Epoch 797, Train MAE: 21.8048\n",
            "Epoch 798, Train MAE: 22.2426\n",
            "Epoch 799, Train MAE: 22.4215\n",
            "Epoch 800, Train MAE: 22.1654\n",
            "Epoch 801, Train MAE: 22.5847\n",
            "Epoch 802, Train MAE: 23.0510\n",
            "Epoch 803, Train MAE: 22.4053\n",
            "Epoch 804, Train MAE: 22.4777\n",
            "Epoch 805, Train MAE: 22.2102\n",
            "Epoch 806, Train MAE: 22.3036\n",
            "Epoch 807, Train MAE: 21.7992\n",
            "Epoch 808, Train MAE: 22.0151\n",
            "Epoch 809, Train MAE: 21.6286\n",
            "Epoch 810, Train MAE: 20.5360\n",
            "Epoch 811, Train MAE: 22.3913\n",
            "Epoch 812, Train MAE: 22.6978\n",
            "Epoch 813, Train MAE: 22.5018\n",
            "Epoch 814, Train MAE: 22.2771\n",
            "Epoch 815, Train MAE: 22.2744\n",
            "Epoch 816, Train MAE: 21.7771\n",
            "Epoch 817, Train MAE: 22.2091\n",
            "Epoch 818, Train MAE: 21.8264\n",
            "Epoch 819, Train MAE: 22.1437\n",
            "Epoch 820, Train MAE: 21.7512\n",
            "Epoch 821, Train MAE: 22.7251\n",
            "Epoch 822, Train MAE: 21.3070\n",
            "Epoch 823, Train MAE: 21.2367\n",
            "Epoch 824, Train MAE: 22.3301\n",
            "Epoch 825, Train MAE: 22.3961\n",
            "Epoch 826, Train MAE: 21.7188\n",
            "Epoch 827, Train MAE: 22.9163\n",
            "Epoch 828, Train MAE: 21.8143\n",
            "Epoch 829, Train MAE: 21.7929\n",
            "Epoch 830, Train MAE: 21.3050\n",
            "Epoch 831, Train MAE: 22.6916\n",
            "Epoch 832, Train MAE: 21.7342\n",
            "Epoch 833, Train MAE: 22.0434\n",
            "Epoch 834, Train MAE: 22.1035\n",
            "Epoch 835, Train MAE: 22.9724\n",
            "Epoch 836, Train MAE: 21.7071\n",
            "Epoch 837, Train MAE: 21.3158\n",
            "Epoch 838, Train MAE: 22.1603\n",
            "Epoch 839, Train MAE: 21.7615\n",
            "Epoch 840, Train MAE: 21.8933\n",
            "Epoch 841, Train MAE: 21.6984\n",
            "Epoch 842, Train MAE: 22.3117\n",
            "Epoch 843, Train MAE: 22.0554\n",
            "Epoch 844, Train MAE: 23.0409\n",
            "Epoch 845, Train MAE: 20.8761\n",
            "Epoch 846, Train MAE: 21.3728\n",
            "Epoch 847, Train MAE: 22.2550\n",
            "Epoch 848, Train MAE: 21.7046\n",
            "Epoch 849, Train MAE: 22.3268\n",
            "Epoch 850, Train MAE: 21.5642\n",
            "Epoch 851, Train MAE: 22.1868\n",
            "Epoch 852, Train MAE: 21.6457\n",
            "Epoch 853, Train MAE: 22.9309\n",
            "Epoch 854, Train MAE: 22.0975\n",
            "Epoch 855, Train MAE: 22.2191\n",
            "Epoch 856, Train MAE: 22.8472\n",
            "Epoch 857, Train MAE: 22.8483\n",
            "Epoch 858, Train MAE: 22.6849\n",
            "Epoch 859, Train MAE: 22.0844\n",
            "Epoch 860, Train MAE: 21.6097\n",
            "Epoch 861, Train MAE: 22.4061\n",
            "Epoch 862, Train MAE: 22.5323\n",
            "Epoch 863, Train MAE: 22.9517\n",
            "Epoch 864, Train MAE: 22.3629\n",
            "Epoch 865, Train MAE: 21.9575\n",
            "Epoch 866, Train MAE: 21.5789\n",
            "Epoch 867, Train MAE: 22.7823\n",
            "Epoch 868, Train MAE: 22.9302\n",
            "Epoch 869, Train MAE: 22.2415\n",
            "Epoch 870, Train MAE: 21.9654\n",
            "Epoch 871, Train MAE: 21.8661\n",
            "Epoch 872, Train MAE: 22.2444\n",
            "Epoch 873, Train MAE: 21.8473\n",
            "Epoch 874, Train MAE: 21.3970\n",
            "Epoch 875, Train MAE: 22.1024\n",
            "Epoch 876, Train MAE: 22.5080\n",
            "Epoch 877, Train MAE: 21.8429\n",
            "Epoch 878, Train MAE: 22.1621\n",
            "Epoch 879, Train MAE: 21.8145\n",
            "Epoch 880, Train MAE: 22.9530\n",
            "Epoch 881, Train MAE: 23.2782\n",
            "Epoch 882, Train MAE: 22.3649\n",
            "Epoch 883, Train MAE: 22.4184\n",
            "Epoch 884, Train MAE: 21.9677\n",
            "Epoch 885, Train MAE: 22.1066\n",
            "Epoch 886, Train MAE: 21.3558\n",
            "Epoch 887, Train MAE: 21.5760\n",
            "Epoch 888, Train MAE: 22.6877\n",
            "Epoch 889, Train MAE: 22.2060\n",
            "Epoch 890, Train MAE: 22.1669\n",
            "Epoch 891, Train MAE: 22.7403\n",
            "Epoch 892, Train MAE: 23.2940\n",
            "Epoch 893, Train MAE: 22.3760\n",
            "Epoch 894, Train MAE: 22.2994\n",
            "Epoch 895, Train MAE: 22.2696\n",
            "Epoch 896, Train MAE: 22.9378\n",
            "Epoch 897, Train MAE: 22.2271\n",
            "Epoch 898, Train MAE: 22.7036\n",
            "Epoch 899, Train MAE: 21.9158\n",
            "Epoch 900, Train MAE: 21.9067\n",
            "Epoch 901, Train MAE: 22.7557\n",
            "Epoch 902, Train MAE: 21.2730\n",
            "Epoch 903, Train MAE: 21.9784\n",
            "Epoch 904, Train MAE: 22.2784\n",
            "Epoch 905, Train MAE: 21.6934\n",
            "Epoch 906, Train MAE: 21.5821\n",
            "Epoch 907, Train MAE: 21.6131\n",
            "Epoch 908, Train MAE: 22.3975\n",
            "Epoch 909, Train MAE: 22.6080\n",
            "Epoch 910, Train MAE: 22.0065\n",
            "Epoch 911, Train MAE: 21.5894\n",
            "Epoch 912, Train MAE: 22.2161\n",
            "Epoch 913, Train MAE: 22.2724\n",
            "Epoch 914, Train MAE: 22.3728\n",
            "Epoch 915, Train MAE: 22.6300\n",
            "Epoch 916, Train MAE: 21.1855\n",
            "Epoch 917, Train MAE: 23.3242\n",
            "Epoch 918, Train MAE: 21.5964\n",
            "Epoch 919, Train MAE: 22.6452\n",
            "Epoch 920, Train MAE: 22.4542\n",
            "Epoch 921, Train MAE: 21.8951\n",
            "Epoch 922, Train MAE: 22.9355\n",
            "Epoch 923, Train MAE: 21.9114\n",
            "Epoch 924, Train MAE: 21.0941\n",
            "Epoch 925, Train MAE: 22.4678\n",
            "Epoch 926, Train MAE: 22.5008\n",
            "Epoch 927, Train MAE: 21.9149\n",
            "Epoch 928, Train MAE: 21.3381\n",
            "Epoch 929, Train MAE: 22.5311\n",
            "Epoch 930, Train MAE: 22.4403\n",
            "Epoch 931, Train MAE: 23.3202\n",
            "Epoch 932, Train MAE: 21.7970\n",
            "Epoch 933, Train MAE: 22.6394\n",
            "Epoch 934, Train MAE: 22.6054\n",
            "Epoch 935, Train MAE: 21.5798\n",
            "Epoch 936, Train MAE: 21.4500\n",
            "Epoch 937, Train MAE: 22.3685\n",
            "Epoch 938, Train MAE: 22.3382\n",
            "Epoch 939, Train MAE: 23.6612\n",
            "Epoch 940, Train MAE: 21.4654\n",
            "Epoch 941, Train MAE: 21.7342\n",
            "Epoch 942, Train MAE: 21.9654\n",
            "Epoch 943, Train MAE: 21.7396\n",
            "Epoch 944, Train MAE: 21.8822\n",
            "Epoch 945, Train MAE: 22.2784\n",
            "Epoch 946, Train MAE: 22.2955\n",
            "Epoch 947, Train MAE: 21.5359\n",
            "Epoch 948, Train MAE: 21.9041\n",
            "Epoch 949, Train MAE: 23.1752\n",
            "Epoch 950, Train MAE: 21.2959\n",
            "Epoch 951, Train MAE: 22.2439\n",
            "Epoch 952, Train MAE: 21.8433\n",
            "Epoch 953, Train MAE: 23.4198\n",
            "Epoch 954, Train MAE: 22.0888\n",
            "Epoch 955, Train MAE: 22.4092\n",
            "Epoch 956, Train MAE: 22.3362\n",
            "Epoch 957, Train MAE: 22.2580\n",
            "Epoch 958, Train MAE: 22.3419\n",
            "Epoch 959, Train MAE: 22.9817\n",
            "Epoch 960, Train MAE: 21.7231\n",
            "Epoch 961, Train MAE: 21.5824\n",
            "Epoch 962, Train MAE: 21.1067\n",
            "Epoch 963, Train MAE: 21.6221\n",
            "Epoch 964, Train MAE: 22.3799\n",
            "Epoch 965, Train MAE: 21.5362\n",
            "Epoch 966, Train MAE: 21.4621\n",
            "Epoch 967, Train MAE: 21.4312\n",
            "Epoch 968, Train MAE: 22.2194\n",
            "Epoch 969, Train MAE: 22.1664\n",
            "Epoch 970, Train MAE: 21.9997\n",
            "Epoch 971, Train MAE: 21.8687\n",
            "Epoch 972, Train MAE: 21.6818\n",
            "Epoch 973, Train MAE: 21.3674\n",
            "Epoch 974, Train MAE: 22.3554\n",
            "Epoch 975, Train MAE: 22.5053\n",
            "Epoch 976, Train MAE: 22.0255\n",
            "Epoch 977, Train MAE: 21.0799\n",
            "Epoch 978, Train MAE: 22.0131\n",
            "Epoch 979, Train MAE: 22.4629\n",
            "Epoch 980, Train MAE: 22.3865\n",
            "Epoch 981, Train MAE: 22.4345\n",
            "Epoch 982, Train MAE: 21.5560\n",
            "Epoch 983, Train MAE: 21.7858\n",
            "Epoch 984, Train MAE: 22.2113\n",
            "Epoch 985, Train MAE: 22.3692\n",
            "Epoch 986, Train MAE: 22.1248\n",
            "Epoch 987, Train MAE: 22.1624\n",
            "Epoch 988, Train MAE: 22.3821\n",
            "Epoch 989, Train MAE: 23.0334\n",
            "Epoch 990, Train MAE: 22.8949\n",
            "Epoch 991, Train MAE: 22.8782\n",
            "Epoch 992, Train MAE: 21.8145\n",
            "Epoch 993, Train MAE: 22.0729\n",
            "Epoch 994, Train MAE: 21.6596\n",
            "Epoch 995, Train MAE: 21.3965\n",
            "Epoch 996, Train MAE: 21.8454\n",
            "Epoch 997, Train MAE: 21.9594\n",
            "Epoch 998, Train MAE: 22.7139\n",
            "Epoch 999, Train MAE: 22.2920\n",
            "Epoch 1000, Train MAE: 21.8317\n",
            "Best Train MAE: 15.552128\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Use TemporalTransformer on aggregated features to predict return_on_asset\n",
        "\n",
        "\n",
        "# Prepare a dataset for tabular (aggregated) features\n",
        "class TabularAggDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.X[idx], self.y[idx]\n",
        "        else:\n",
        "            return self.X[idx]\n",
        "\n",
        "# Use only the features (no target) for test set\n",
        "X_test = test_agg_imputed.values.astype(np.float32)\n",
        "test_ds = TabularAggDataset(test_agg_imputed)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# For training, use X_train_imputed and y_train\n",
        "X_train = X_train_imputed.values.astype(np.float32)\n",
        "y_train_tensor = y_train.values.astype(np.float32)\n",
        "train_ds = TabularAggDataset(X_train_imputed, y_train)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define a simple transformer for tabular data\n",
        "import torch.nn as nn\n",
        "\n",
        "class TabularTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(d_model, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, features)\n",
        "        x = self.input_proj(x).unsqueeze(1)  # (batch, 1, d_model)\n",
        "        x = self.transformer(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TabularTransformer(input_dim=X_train.shape[1]).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "best_mae = float('inf')\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb).squeeze()\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation on train set (for simplicity)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            pred = model(xb).squeeze().cpu().numpy()\n",
        "            preds.append(pred)\n",
        "        preds = np.concatenate([p.reshape(-1) for p in preds])\n",
        "        mae = np.mean(np.abs(preds - y_train_tensor))\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "    print(f\"Epoch {epoch+1}, Train MAE: {mae:.4f}\")\n",
        "\n",
        "print(\"Best Train MAE:\", best_mae)\n",
        "\n",
        "# Predict on test set\n",
        "model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).squeeze().cpu().numpy()\n",
        "        test_preds.append(pred)\n",
        "test_preds = np.concatenate([p.reshape(-1) for p in test_preds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2de6e0aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest on Transformer Embeddings CV MAE: [12.49126214 13.44857843 12.25112745 10.81632353 13.26681373]\n",
            "Mean MAE: 12.454821054635445\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Step 1: Get transformer embeddings for train and test sets\n",
        "# We'll use the output of the penultimate layer (before the regression head) as features\n",
        "\n",
        "def get_transformer_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            # Forward pass up to the transformer encoder\n",
        "            x_proj = model.input_proj(xb)\n",
        "            x_enc = model.transformer(x_proj)\n",
        "            # Flatten for each sample\n",
        "            emb = x_enc.reshape(x_enc.shape[0], -1).cpu().numpy()\n",
        "            embeddings.append(emb)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Prepare train sequence dataset and loader\n",
        "seq_len = 32  # Use the same as in previous cells\n",
        "feature_cols = [c for c in train_df.columns if c.startswith('indicator_')]\n",
        "train_seq_ds = AssetSequenceDataset(train_df, target_col='return_on_asset', seq_len=seq_len, feature_cols=feature_cols)\n",
        "train_seq_loader = DataLoader(train_seq_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# Prepare test sequence dataset and loader\n",
        "test_seq_ds = AssetSequenceDataset(test_df, target_col=None, seq_len=seq_len, feature_cols=feature_cols)\n",
        "test_seq_loader = DataLoader(test_seq_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define and (optionally) train the temporal_model before using it\n",
        "# Assume input_dim is the number of features in your sequence data\n",
        "input_dim = len(feature_cols)\n",
        "temporal_model = TemporalTransformer(input_dim=input_dim, d_model=64, nhead=4, num_layers=2, seq_len=seq_len).to(device)\n",
        "\n",
        "# (Optional) You may want to train the model here before extracting embeddings\n",
        "# For demonstration, we'll just use the randomly initialized model\n",
        "\n",
        "# Get embeddings\n",
        "train_embeddings = get_transformer_embeddings(temporal_model, train_seq_loader, device)\n",
        "test_embeddings = get_transformer_embeddings(temporal_model, test_seq_loader, device)\n",
        "\n",
        "# Step 2: Fit RandomForest on transformer embeddings\n",
        "\n",
        "# Use y_train as target (already aligned with train_seq_ds)\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(train_embeddings, y_train.values)\n",
        "\n",
        "# Cross-validated MAE (optional)\n",
        "cv_mae = -cross_val_score(rf, train_embeddings, y_train.values, cv=5, scoring='neg_mean_absolute_error')\n",
        "print(\"RandomForest on Transformer Embeddings CV MAE:\", cv_mae)\n",
        "print(\"Mean MAE:\", np.mean(cv_mae))\n",
        "\n",
        "# Step 3: Predict on test set\n",
        "test_preds_rf = rf.predict(test_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "359c7396",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "def expanding_window_cv(model, X, y, \n",
        "                        initial_window=1000, \n",
        "                        horizon=200, \n",
        "                        step=200, \n",
        "                        verbose=True):\n",
        "    \"\"\"\n",
        "    Expanding window time series cross-validation.\n",
        "    \n",
        "    Args:\n",
        "        model: scikit-learn-like regressor (with fit/predict).\n",
        "        X: DataFrame or np.ndarray of features.\n",
        "        y: Series or np.ndarray of targets.\n",
        "        initial_window: Size of initial training window.\n",
        "        horizon: Size of each validation/test split.\n",
        "        step: How much to move forward each time.\n",
        "        verbose: Print split info and scores.\n",
        "        \n",
        "    Returns:\n",
        "        List of MAE scores for each fold.\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    cv_scores = []\n",
        "    folds = 0\n",
        "\n",
        "    for start in range(0, n - initial_window - horizon + 1, step):\n",
        "        train_end = start + initial_window\n",
        "        test_end = train_end + horizon\n",
        "\n",
        "        X_train = X.iloc[start:train_end]\n",
        "        y_train = y.iloc[start:train_end]\n",
        "        X_val = X.iloc[train_end:test_end]\n",
        "        y_val = y.iloc[train_end:test_end]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_val)\n",
        "        score = mean_absolute_error(y_val, preds)\n",
        "        cv_scores.append(score)\n",
        "        folds += 1\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[Fold {folds}] Train: {start}:{train_end}, Test: {train_end}:{test_end}, MAE: {score:.4f}\")\n",
        "\n",
        "    print(\"\\n Expanding Window CV Complete\")\n",
        "    print(f\"Folds run: {folds}\")\n",
        "    print(f\"MAE: mean={np.mean(cv_scores):.4f}, std={np.std(cv_scores):.4f}\")\n",
        "    return cv_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1df90ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAE scores: [8.001262135922332, 7.372254901960783, 7.230686274509804, 6.949901960784314, 7.492450980392157]\n",
            "Average MAE: 7.409311250713879\n"
          ]
        }
      ],
      "source": [
        "print(\"CV MAE scores:\", cv_scores)\n",
        "print(\"Average MAE:\", np.mean(cv_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb3be20",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2b3f6f60",
      "metadata": {
        "id": "2b3f6f60",
        "outputId": "fbcdd781-3f7a-4556-a0d7-b8060598cfec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabularTransformer Train MAE: 21.540485367149756\n",
            "TabularTransformer Test predictions shape: (315,)\n",
            "RandomForest Embedding Train MAE: 5.26165362035225\n",
            "RandomForest Embedding Test predictions shape: (315,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Compute final MAE on train and test sets for both models\n",
        "\n",
        "# For TabularTransformer predictions (train set only, since test set targets are known)\n",
        "model.eval()\n",
        "tabular_train_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).squeeze().cpu().numpy()\n",
        "        tabular_train_preds.append(pred)\n",
        "tabular_train_preds = np.concatenate([p.reshape(-1) for p in tabular_train_preds])\n",
        "\n",
        "# Remove NaNs from both y_train and tabular_train_preds before computing MAE\n",
        "mask = ~(\n",
        "    pd.isna(y_train.values) | \n",
        "    pd.isna(tabular_train_preds)\n",
        ")\n",
        "mae_train_tabular = mean_absolute_error(y_train.values[mask], tabular_train_preds[mask])\n",
        "print(\"TabularTransformer Train MAE:\", mae_train_tabular)\n",
        "\n",
        "# Test predictions (TabularTransformer)\n",
        "# Since test set has no targets, just show shape\n",
        "model.eval()\n",
        "tabular_test_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).squeeze().cpu().numpy()\n",
        "        tabular_test_preds.append(pred)\n",
        "tabular_test_preds = np.concatenate([p.reshape(-1) for p in tabular_test_preds])\n",
        "print(\"TabularTransformer Test predictions shape:\", tabular_test_preds.shape)\n",
        "\n",
        "# For RandomForest on Transformer Embeddings (test_preds_rf)\n",
        "# Train set\n",
        "rf_train_preds = rf.predict(train_embeddings)\n",
        "mae_train_rf = mean_absolute_error(y_train, rf_train_preds)\n",
        "print(\"RandomForest Embedding Train MAE:\", mae_train_rf)\n",
        "\n",
        "# Test set\n",
        "print(\"RandomForest Embedding Test predictions shape:\", test_preds_rf.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8db4a174",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble predictions (bagging + boosting) computed.\n"
          ]
        }
      ],
      "source": [
        "# Combine predictions from bagging (RandomForest) and boosting (GradientBoostingRegressor) to reduce variance\n",
        "\n",
        "# Average predictions from both models (already computed as rf_preds and gbr_preds)\n",
        "ensemble_preds = 0.5 * rf_preds + 0.5 * gbr_preds\n",
        "\n",
        "# Optionally, evaluate ensemble performance on train set (if you have validation targets)\n",
        "# Example (if you have y_val and val_preds from ensemble):\n",
        "# mae_ensemble = mean_absolute_error(y_val, ensemble_preds_on_val)\n",
        "\n",
        "print(\"Ensemble predictions (bagging + boosting) computed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "015fac4d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TemporalTransformer Train MAE: 21.540485367149756\n",
            "TemporalTransformer Test predictions shape: (315,)\n",
            "RandomForest Embedding Train MAE: 5.26165362035225\n",
            "RandomForest Embedding Test predictions shape: (315,)\n"
          ]
        }
      ],
      "source": [
        "print(\"TemporalTransformer Train MAE:\", mae_train_tabular)\n",
        "print(\"TemporalTransformer Test predictions shape:\", tabular_test_preds.shape)\n",
        "print(\"RandomForest Embedding Train MAE:\", mae_train_rf)\n",
        "# Test set\n",
        "print(\"RandomForest Embedding Test predictions shape:\", test_preds_rf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6b974c4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file saved as kaggle_submission.csv\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Create submission file\n",
        "submission = sample_submission_df.copy()\n",
        "submission['return_on_asset'] = submission['asset_id'].map(\n",
        "    dict(zip(test_agg.index, test_preds))\n",
        ").fillna(0)\n",
        "\n",
        "# Save the CSV file\n",
        "submission.to_csv(\"submission_simple_Temporal_randomF_2.csv\", index=False)\n",
        "print(\"Submission file saved as kaggle_submission.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
