{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa8c7e03",
      "metadata": {
        "id": "fa8c7e03"
      },
      "source": [
        "# MGTF 424 Final Project\n",
        "This notebook builds a baseline model to predict `return_on_asset` using panel data with anonymized indicators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5140c0d2",
      "metadata": {
        "id": "5140c0d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "sample_submission_df = pd.read_csv(\"sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4edc1d69",
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Remove outliers from train_df based on return_on_asset (e.g., 1st and 99th percentiles)\n",
        "# q_low = train_df['return_on_asset'].quantile(0.01)\n",
        "# q_high = train_df['return_on_asset'].quantile(0.99)\n",
        "# train_df_clean = train_df[(train_df['return_on_asset'] >= q_low) & (train_df['return_on_asset'] <= q_high)].copy()\n",
        "\n",
        "# # Normalize target\n",
        "# scaler = StandardScaler()\n",
        "# train_df_clean['return_on_asset_norm'] = scaler.fit_transform(train_df_clean[['return_on_asset']])\n",
        "\n",
        "# # Add lag features and rolling stats for each asset_id\n",
        "# lag_features = []\n",
        "# rolling_features = []\n",
        "# window = 3\n",
        "\n",
        "# for col in [c for c in train_df_clean.columns if c.startswith('indicator_')]:\n",
        "#     # Lag 1\n",
        "#     lag_col = f\"{col}_lag1\"\n",
        "#     train_df_clean[lag_col] = train_df_clean.groupby('asset_id')[col].shift(1)\n",
        "#     lag_features.append(lag_col)\n",
        "#     # Rolling mean\n",
        "#     roll_mean_col = f\"{col}_roll{window}_mean\"\n",
        "#     train_df_clean[roll_mean_col] = train_df_clean.groupby('asset_id')[col].rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "#     rolling_features.append(roll_mean_col)\n",
        "#     # Rolling std\n",
        "#     roll_std_col = f\"{col}_roll{window}_std\"\n",
        "#     train_df_clean[roll_std_col] = train_df_clean.groupby('asset_id')[col].rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "#     rolling_features.append(roll_std_col)\n",
        "\n",
        "# # Drop rows with NaN after lagging (optional, or impute later)\n",
        "# train_df_clean = train_df_clean.dropna(subset=lag_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "899c0baf",
      "metadata": {
        "id": "899c0baf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Aggregate features\n",
        "def aggregate_features(df, is_train=True):\n",
        "    agg_funcs = ['mean', 'std', 'min', 'max']\n",
        "    feature_cols = [col for col in df.columns if col.startswith(\"indicator_\")]\n",
        "\n",
        "    aggregated = df.groupby('asset_id')[feature_cols].agg(agg_funcs)\n",
        "    aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\n",
        "\n",
        "    if is_train:\n",
        "        static_cols = ['return_on_asset', 'company_age', 'company_size', 'revenue']\n",
        "    else:\n",
        "        static_cols = ['company_age', 'company_size', 'revenue']\n",
        "\n",
        "    static_info = df.groupby('asset_id')[static_cols].first()\n",
        "    return aggregated.join(static_info)\n",
        "\n",
        "train_agg = aggregate_features(train_df, is_train=True)\n",
        "test_agg = aggregate_features(test_df, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c89e92a4",
      "metadata": {
        "id": "c89e92a4"
      },
      "outputs": [],
      "source": [
        "# Step 2: Prepare training data\n",
        "X_train = train_agg.drop(columns=[\"return_on_asset\"])\n",
        "y_train = train_agg[\"return_on_asset\"]\n",
        "groups = train_agg.index\n",
        "\n",
        "# Step 3: Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "test_agg_imputed = pd.DataFrame(imputer.transform(test_agg), index=test_agg.index, columns=test_agg.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fa8e1802",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = train_agg[\"return_on_asset\"]\n",
        "y_train = np.clip(y_train, a_min=-100, a_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03aefd0a",
      "metadata": {
        "id": "03aefd0a",
        "outputId": "f62d8483-403b-4923-dd18-92a31bce2194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAE scores: [8.135922330097088, 7.294019607843136, 7.232156862745098, 7.036274509803922, 7.384607843137253]\n",
            "Average MAE: 7.4165962307252995\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Cross-validated training\n",
        "\n",
        "# Ensure X_train_imputed is defined\n",
        "if 'X_train_imputed' not in globals():\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train = train_agg.drop(columns=[\"return_on_asset\"])\n",
        "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "\n",
        "cv = GroupKFold(n_splits=5)\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
        "cv_scores = []\n",
        "\n",
        "# Use 'groups' as defined in cell 4 (groups = train_agg.index)\n",
        "for train_idx, val_idx in cv.split(X_train_imputed, y_train, groups):\n",
        "    X_tr, X_val = X_train_imputed.iloc[train_idx], X_train_imputed.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "    preds = model.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, preds)\n",
        "    cv_scores.append(score)\n",
        "\n",
        "print(\"CV MAE scores:\", cv_scores)\n",
        "print(\"Average MAE:\", np.mean(cv_scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "73dae0be",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Prepare sequence data for each asset_id\n",
        "class AssetSequenceDataset(Dataset):\n",
        "    def __init__(self, df, target_col, seq_len=32, feature_cols=None):\n",
        "        self.groups = []\n",
        "        self.sequences = []\n",
        "        self.targets = []\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        if feature_cols is None:\n",
        "            feature_cols = [c for c in df.columns if c.startswith('indicator_')]\n",
        "        self.feature_cols = feature_cols\n",
        "\n",
        "        grouped = df.groupby('asset_id')\n",
        "        for asset_id, group in grouped:\n",
        "            group = group.sort_values('timestamp')\n",
        "            features = group[self.feature_cols].values\n",
        "            target = group[target_col].values if target_col in group else None\n",
        "            # Pad or truncate\n",
        "            if len(features) < seq_len:\n",
        "                pad_width = seq_len - len(features)\n",
        "                features = np.pad(features, ((pad_width,0),(0,0)), 'constant')\n",
        "                if target is not None:\n",
        "                    target = np.pad(target, (pad_width,0), 'constant')\n",
        "            else:\n",
        "                features = features[-seq_len:]\n",
        "                if target is not None:\n",
        "                    target = target[-seq_len:]\n",
        "            self.sequences.append(features)\n",
        "            if target is not None:\n",
        "                self.targets.append(target[-1])  # predict last value\n",
        "            self.groups.append(asset_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.targets[idx], dtype=torch.float32) if self.targets else torch.tensor(0.0)\n",
        "        return x, y\n",
        "\n",
        "# Temporal Transformer Model\n",
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, seq_len=32):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(d_model * seq_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.transformer(x)\n",
        "        return self.head(x)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8cc0de3e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAE scores: [8.135922330097088, 7.294019607843136, 7.232156862745098, 7.036274509803922, 7.384607843137253]\n",
            "Average MAE: 7.4165962307252995\n"
          ]
        }
      ],
      "source": [
        "print(\"CV MAE scores:\", cv_scores)\n",
        "print(\"Average MAE:\", np.mean(cv_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1aca2e6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train MAE: 42.7811\n",
            "Epoch 2, Train MAE: 41.7912\n",
            "Epoch 3, Train MAE: 40.4059\n",
            "Epoch 4, Train MAE: 38.4099\n",
            "Epoch 5, Train MAE: 35.7605\n",
            "Epoch 6, Train MAE: 32.3637\n",
            "Epoch 7, Train MAE: 28.1732\n",
            "Epoch 8, Train MAE: 23.5077\n",
            "Epoch 9, Train MAE: 19.4445\n",
            "Epoch 10, Train MAE: 16.8237\n",
            "Epoch 11, Train MAE: 15.6708\n",
            "Epoch 12, Train MAE: 15.5329\n",
            "Epoch 13, Train MAE: 15.9844\n",
            "Epoch 14, Train MAE: 16.9048\n",
            "Epoch 15, Train MAE: 17.0939\n",
            "Epoch 16, Train MAE: 17.0719\n",
            "Epoch 17, Train MAE: 16.8157\n",
            "Epoch 18, Train MAE: 18.4560\n",
            "Epoch 19, Train MAE: 18.5953\n",
            "Epoch 20, Train MAE: 17.6262\n",
            "Epoch 21, Train MAE: 18.5178\n",
            "Epoch 22, Train MAE: 19.4617\n",
            "Epoch 23, Train MAE: 18.6489\n",
            "Epoch 24, Train MAE: 20.4509\n",
            "Epoch 25, Train MAE: 20.0274\n",
            "Epoch 26, Train MAE: 18.1570\n",
            "Epoch 27, Train MAE: 21.0977\n",
            "Epoch 28, Train MAE: 18.9685\n",
            "Epoch 29, Train MAE: 19.0544\n",
            "Epoch 30, Train MAE: 21.0080\n",
            "Epoch 31, Train MAE: 18.4218\n",
            "Epoch 32, Train MAE: 20.6532\n",
            "Epoch 33, Train MAE: 19.4987\n",
            "Epoch 34, Train MAE: 20.8378\n",
            "Epoch 35, Train MAE: 19.8072\n",
            "Epoch 36, Train MAE: 19.6515\n",
            "Epoch 37, Train MAE: 20.0478\n",
            "Epoch 38, Train MAE: 19.5595\n",
            "Epoch 39, Train MAE: 20.8285\n",
            "Epoch 40, Train MAE: 19.7374\n",
            "Epoch 41, Train MAE: 19.3798\n",
            "Epoch 42, Train MAE: 20.3555\n",
            "Epoch 43, Train MAE: 21.1122\n",
            "Epoch 44, Train MAE: 19.6231\n",
            "Epoch 45, Train MAE: 20.0282\n",
            "Epoch 46, Train MAE: 20.1967\n",
            "Epoch 47, Train MAE: 19.4796\n",
            "Epoch 48, Train MAE: 20.8072\n",
            "Epoch 49, Train MAE: 20.7791\n",
            "Epoch 50, Train MAE: 21.3120\n",
            "Epoch 51, Train MAE: 21.4048\n",
            "Epoch 52, Train MAE: 21.4352\n",
            "Epoch 53, Train MAE: 21.5092\n",
            "Epoch 54, Train MAE: 20.4968\n",
            "Epoch 55, Train MAE: 20.1119\n",
            "Epoch 56, Train MAE: 22.0773\n",
            "Epoch 57, Train MAE: 20.8941\n",
            "Epoch 58, Train MAE: 20.3071\n",
            "Epoch 59, Train MAE: 21.7105\n",
            "Epoch 60, Train MAE: 22.1985\n",
            "Epoch 61, Train MAE: 20.1429\n",
            "Epoch 62, Train MAE: 21.7348\n",
            "Epoch 63, Train MAE: 22.5753\n",
            "Epoch 64, Train MAE: 20.8002\n",
            "Epoch 65, Train MAE: 22.5533\n",
            "Epoch 66, Train MAE: 21.6906\n",
            "Epoch 67, Train MAE: 22.1711\n",
            "Epoch 68, Train MAE: 22.9710\n",
            "Epoch 69, Train MAE: 20.5692\n",
            "Epoch 70, Train MAE: 21.2568\n",
            "Epoch 71, Train MAE: 22.2432\n",
            "Epoch 72, Train MAE: 21.4023\n",
            "Epoch 73, Train MAE: 21.4440\n",
            "Epoch 74, Train MAE: 21.9050\n",
            "Epoch 75, Train MAE: 20.9370\n",
            "Epoch 76, Train MAE: 22.1411\n",
            "Epoch 77, Train MAE: 21.7643\n",
            "Epoch 78, Train MAE: 20.8282\n",
            "Epoch 79, Train MAE: 21.6221\n",
            "Epoch 80, Train MAE: 22.7822\n",
            "Epoch 81, Train MAE: 22.8580\n",
            "Epoch 82, Train MAE: 22.5082\n",
            "Epoch 83, Train MAE: 22.5972\n",
            "Epoch 84, Train MAE: 22.4382\n",
            "Epoch 85, Train MAE: 21.9052\n",
            "Epoch 86, Train MAE: 20.8201\n",
            "Epoch 87, Train MAE: 22.4414\n",
            "Epoch 88, Train MAE: 21.5017\n",
            "Epoch 89, Train MAE: 22.3830\n",
            "Epoch 90, Train MAE: 21.9206\n",
            "Epoch 91, Train MAE: 20.5990\n",
            "Epoch 92, Train MAE: 22.3131\n",
            "Epoch 93, Train MAE: 22.2275\n",
            "Epoch 94, Train MAE: 21.5268\n",
            "Epoch 95, Train MAE: 21.5977\n",
            "Epoch 96, Train MAE: 21.3570\n",
            "Epoch 97, Train MAE: 22.3455\n",
            "Epoch 98, Train MAE: 21.9332\n",
            "Epoch 99, Train MAE: 23.0655\n",
            "Epoch 100, Train MAE: 22.7982\n",
            "Epoch 101, Train MAE: 22.2902\n",
            "Epoch 102, Train MAE: 23.1260\n",
            "Epoch 103, Train MAE: 21.9182\n",
            "Epoch 104, Train MAE: 21.1376\n",
            "Epoch 105, Train MAE: 22.1916\n",
            "Epoch 106, Train MAE: 21.3331\n",
            "Epoch 107, Train MAE: 22.2943\n",
            "Epoch 108, Train MAE: 21.9858\n",
            "Epoch 109, Train MAE: 21.7450\n",
            "Epoch 110, Train MAE: 22.2547\n",
            "Epoch 111, Train MAE: 21.6697\n",
            "Epoch 112, Train MAE: 22.0492\n",
            "Epoch 113, Train MAE: 22.1286\n",
            "Epoch 114, Train MAE: 21.8183\n",
            "Epoch 115, Train MAE: 21.8937\n",
            "Epoch 116, Train MAE: 23.1789\n",
            "Epoch 117, Train MAE: 22.2722\n",
            "Epoch 118, Train MAE: 21.0927\n",
            "Epoch 119, Train MAE: 22.5139\n",
            "Epoch 120, Train MAE: 22.5579\n",
            "Epoch 121, Train MAE: 21.9658\n",
            "Epoch 122, Train MAE: 21.4227\n",
            "Epoch 123, Train MAE: 22.3641\n",
            "Epoch 124, Train MAE: 22.7362\n",
            "Epoch 125, Train MAE: 22.2425\n",
            "Epoch 126, Train MAE: 21.4569\n",
            "Epoch 127, Train MAE: 22.7213\n",
            "Epoch 128, Train MAE: 22.3861\n",
            "Epoch 129, Train MAE: 22.4142\n",
            "Epoch 130, Train MAE: 20.9514\n",
            "Epoch 131, Train MAE: 21.8706\n",
            "Epoch 132, Train MAE: 22.7242\n",
            "Epoch 133, Train MAE: 22.3356\n",
            "Epoch 134, Train MAE: 20.4621\n",
            "Epoch 135, Train MAE: 22.5174\n",
            "Epoch 136, Train MAE: 22.3395\n",
            "Epoch 137, Train MAE: 22.2469\n",
            "Epoch 138, Train MAE: 22.3798\n",
            "Epoch 139, Train MAE: 22.0664\n",
            "Epoch 140, Train MAE: 20.9987\n",
            "Epoch 141, Train MAE: 22.0890\n",
            "Epoch 142, Train MAE: 22.3003\n",
            "Epoch 143, Train MAE: 20.2744\n",
            "Epoch 144, Train MAE: 23.2477\n",
            "Epoch 145, Train MAE: 22.0072\n",
            "Epoch 146, Train MAE: 21.7655\n",
            "Epoch 147, Train MAE: 22.3766\n",
            "Epoch 148, Train MAE: 22.0125\n",
            "Epoch 149, Train MAE: 23.0395\n",
            "Epoch 150, Train MAE: 22.2974\n",
            "Epoch 151, Train MAE: 21.2123\n",
            "Epoch 152, Train MAE: 22.4430\n",
            "Epoch 153, Train MAE: 22.1093\n",
            "Epoch 154, Train MAE: 21.9590\n",
            "Epoch 155, Train MAE: 21.5985\n",
            "Epoch 156, Train MAE: 22.1901\n",
            "Epoch 157, Train MAE: 23.4962\n",
            "Epoch 158, Train MAE: 21.0153\n",
            "Epoch 159, Train MAE: 22.3324\n",
            "Epoch 160, Train MAE: 22.4915\n",
            "Epoch 161, Train MAE: 21.6773\n",
            "Epoch 162, Train MAE: 22.4397\n",
            "Epoch 163, Train MAE: 22.5540\n",
            "Epoch 164, Train MAE: 21.6006\n",
            "Epoch 165, Train MAE: 21.9724\n",
            "Epoch 166, Train MAE: 22.0462\n",
            "Epoch 167, Train MAE: 22.5449\n",
            "Epoch 168, Train MAE: 22.0378\n",
            "Epoch 169, Train MAE: 22.3065\n",
            "Epoch 170, Train MAE: 21.8225\n",
            "Epoch 171, Train MAE: 23.2432\n",
            "Epoch 172, Train MAE: 22.2974\n",
            "Epoch 173, Train MAE: 22.4381\n",
            "Epoch 174, Train MAE: 22.3522\n",
            "Epoch 175, Train MAE: 21.3225\n",
            "Epoch 176, Train MAE: 21.6582\n",
            "Epoch 177, Train MAE: 22.3025\n",
            "Epoch 178, Train MAE: 21.8011\n",
            "Epoch 179, Train MAE: 21.0631\n",
            "Epoch 180, Train MAE: 21.4844\n",
            "Epoch 181, Train MAE: 21.6673\n",
            "Epoch 182, Train MAE: 21.9587\n",
            "Epoch 183, Train MAE: 22.2835\n",
            "Epoch 184, Train MAE: 21.4606\n",
            "Epoch 185, Train MAE: 21.6641\n",
            "Epoch 186, Train MAE: 22.6117\n",
            "Epoch 187, Train MAE: 21.8990\n",
            "Epoch 188, Train MAE: 21.5929\n",
            "Epoch 189, Train MAE: 22.5505\n",
            "Epoch 190, Train MAE: 22.5243\n",
            "Epoch 191, Train MAE: 22.3746\n",
            "Epoch 192, Train MAE: 21.8728\n",
            "Epoch 193, Train MAE: 21.5871\n",
            "Epoch 194, Train MAE: 22.1006\n",
            "Epoch 195, Train MAE: 20.7330\n",
            "Epoch 196, Train MAE: 21.5087\n",
            "Epoch 197, Train MAE: 22.4875\n",
            "Epoch 198, Train MAE: 23.0286\n",
            "Epoch 199, Train MAE: 22.5764\n",
            "Epoch 200, Train MAE: 22.2798\n",
            "Epoch 201, Train MAE: 22.5262\n",
            "Epoch 202, Train MAE: 22.1277\n",
            "Epoch 203, Train MAE: 21.6999\n",
            "Epoch 204, Train MAE: 20.8716\n",
            "Epoch 205, Train MAE: 23.0920\n",
            "Epoch 206, Train MAE: 22.8144\n",
            "Epoch 207, Train MAE: 22.4691\n",
            "Epoch 208, Train MAE: 21.8969\n",
            "Epoch 209, Train MAE: 22.4213\n",
            "Epoch 210, Train MAE: 22.2486\n",
            "Epoch 211, Train MAE: 22.7967\n",
            "Epoch 212, Train MAE: 21.6742\n",
            "Epoch 213, Train MAE: 23.0482\n",
            "Epoch 214, Train MAE: 21.8919\n",
            "Epoch 215, Train MAE: 21.6534\n",
            "Epoch 216, Train MAE: 21.2995\n",
            "Epoch 217, Train MAE: 22.8352\n",
            "Epoch 218, Train MAE: 22.6942\n",
            "Epoch 219, Train MAE: 21.7647\n",
            "Epoch 220, Train MAE: 20.9513\n",
            "Epoch 221, Train MAE: 22.4725\n",
            "Epoch 222, Train MAE: 22.0969\n",
            "Epoch 223, Train MAE: 23.0586\n",
            "Epoch 224, Train MAE: 22.7985\n",
            "Epoch 225, Train MAE: 22.0881\n",
            "Epoch 226, Train MAE: 21.8693\n",
            "Epoch 227, Train MAE: 21.9800\n",
            "Epoch 228, Train MAE: 22.4031\n",
            "Epoch 229, Train MAE: 22.4166\n",
            "Epoch 230, Train MAE: 22.2767\n",
            "Epoch 231, Train MAE: 21.3925\n",
            "Epoch 232, Train MAE: 21.9596\n",
            "Epoch 233, Train MAE: 20.5742\n",
            "Epoch 234, Train MAE: 22.6097\n",
            "Epoch 235, Train MAE: 21.3746\n",
            "Epoch 236, Train MAE: 23.2949\n",
            "Epoch 237, Train MAE: 21.8286\n",
            "Epoch 238, Train MAE: 21.6769\n",
            "Epoch 239, Train MAE: 22.4317\n",
            "Epoch 240, Train MAE: 22.1195\n",
            "Epoch 241, Train MAE: 22.3567\n",
            "Epoch 242, Train MAE: 22.0765\n",
            "Epoch 243, Train MAE: 22.1796\n",
            "Epoch 244, Train MAE: 22.4261\n",
            "Epoch 245, Train MAE: 21.4596\n",
            "Epoch 246, Train MAE: 22.2811\n",
            "Epoch 247, Train MAE: 22.2625\n",
            "Epoch 248, Train MAE: 21.7504\n",
            "Epoch 249, Train MAE: 20.9951\n",
            "Epoch 250, Train MAE: 22.3875\n",
            "Epoch 251, Train MAE: 22.6321\n",
            "Epoch 252, Train MAE: 21.8832\n",
            "Epoch 253, Train MAE: 22.0243\n",
            "Epoch 254, Train MAE: 22.2804\n",
            "Epoch 255, Train MAE: 22.5744\n",
            "Epoch 256, Train MAE: 21.6433\n",
            "Epoch 257, Train MAE: 22.2815\n",
            "Epoch 258, Train MAE: 22.1109\n",
            "Epoch 259, Train MAE: 22.4676\n",
            "Epoch 260, Train MAE: 21.4571\n",
            "Epoch 261, Train MAE: 21.9845\n",
            "Epoch 262, Train MAE: 22.2321\n",
            "Epoch 263, Train MAE: 22.0916\n",
            "Epoch 264, Train MAE: 20.4770\n",
            "Epoch 265, Train MAE: 22.2631\n",
            "Epoch 266, Train MAE: 22.0681\n",
            "Epoch 267, Train MAE: 22.4234\n",
            "Epoch 268, Train MAE: 22.4624\n",
            "Epoch 269, Train MAE: 22.2372\n",
            "Epoch 270, Train MAE: 21.4955\n",
            "Epoch 271, Train MAE: 23.2797\n",
            "Epoch 272, Train MAE: 22.8449\n",
            "Epoch 273, Train MAE: 22.0331\n",
            "Epoch 274, Train MAE: 22.2234\n",
            "Epoch 275, Train MAE: 22.5979\n",
            "Epoch 276, Train MAE: 21.0088\n",
            "Epoch 277, Train MAE: 22.2282\n",
            "Epoch 278, Train MAE: 23.4190\n",
            "Epoch 279, Train MAE: 20.3515\n",
            "Epoch 280, Train MAE: 22.3087\n",
            "Epoch 281, Train MAE: 23.0103\n",
            "Epoch 282, Train MAE: 22.2433\n",
            "Epoch 283, Train MAE: 21.4359\n",
            "Epoch 284, Train MAE: 20.7541\n",
            "Epoch 285, Train MAE: 22.2544\n",
            "Epoch 286, Train MAE: 22.1605\n",
            "Epoch 287, Train MAE: 22.8973\n",
            "Epoch 288, Train MAE: 22.1648\n",
            "Epoch 289, Train MAE: 22.0737\n",
            "Epoch 290, Train MAE: 22.5986\n",
            "Epoch 291, Train MAE: 21.9454\n",
            "Epoch 292, Train MAE: 21.2594\n",
            "Epoch 293, Train MAE: 21.1218\n",
            "Epoch 294, Train MAE: 20.8141\n",
            "Epoch 295, Train MAE: 21.5651\n",
            "Epoch 296, Train MAE: 23.3244\n",
            "Epoch 297, Train MAE: 21.9930\n",
            "Epoch 298, Train MAE: 21.9728\n",
            "Epoch 299, Train MAE: 22.1847\n",
            "Epoch 300, Train MAE: 22.0001\n",
            "Epoch 301, Train MAE: 20.7771\n",
            "Epoch 302, Train MAE: 22.8070\n",
            "Epoch 303, Train MAE: 22.3279\n",
            "Epoch 304, Train MAE: 22.5122\n",
            "Epoch 305, Train MAE: 21.1649\n",
            "Epoch 306, Train MAE: 22.6228\n",
            "Epoch 307, Train MAE: 22.1327\n",
            "Epoch 308, Train MAE: 22.3681\n",
            "Epoch 309, Train MAE: 22.6556\n",
            "Epoch 310, Train MAE: 22.1667\n",
            "Epoch 311, Train MAE: 21.3162\n",
            "Epoch 312, Train MAE: 21.9954\n",
            "Epoch 313, Train MAE: 22.2927\n",
            "Epoch 314, Train MAE: 21.3259\n",
            "Epoch 315, Train MAE: 22.8456\n",
            "Epoch 316, Train MAE: 22.8298\n",
            "Epoch 317, Train MAE: 22.9661\n",
            "Epoch 318, Train MAE: 21.8940\n",
            "Epoch 319, Train MAE: 22.7874\n",
            "Epoch 320, Train MAE: 21.4509\n",
            "Epoch 321, Train MAE: 22.0999\n",
            "Epoch 322, Train MAE: 21.9182\n",
            "Epoch 323, Train MAE: 22.5065\n",
            "Epoch 324, Train MAE: 21.2624\n",
            "Epoch 325, Train MAE: 22.7821\n",
            "Epoch 326, Train MAE: 21.7705\n",
            "Epoch 327, Train MAE: 21.8162\n",
            "Epoch 328, Train MAE: 21.9973\n",
            "Epoch 329, Train MAE: 21.7840\n",
            "Epoch 330, Train MAE: 22.1586\n",
            "Epoch 331, Train MAE: 22.2827\n",
            "Epoch 332, Train MAE: 22.0849\n",
            "Epoch 333, Train MAE: 22.1453\n",
            "Epoch 334, Train MAE: 22.9187\n",
            "Epoch 335, Train MAE: 22.7006\n",
            "Epoch 336, Train MAE: 23.5501\n",
            "Epoch 337, Train MAE: 22.4901\n",
            "Epoch 338, Train MAE: 22.0222\n",
            "Epoch 339, Train MAE: 22.0226\n",
            "Epoch 340, Train MAE: 23.1600\n",
            "Epoch 341, Train MAE: 21.6266\n",
            "Epoch 342, Train MAE: 22.4276\n",
            "Epoch 343, Train MAE: 22.7701\n",
            "Epoch 344, Train MAE: 21.8308\n",
            "Epoch 345, Train MAE: 22.9970\n",
            "Epoch 346, Train MAE: 23.0973\n",
            "Epoch 347, Train MAE: 22.3288\n",
            "Epoch 348, Train MAE: 21.8477\n",
            "Epoch 349, Train MAE: 20.4939\n",
            "Epoch 350, Train MAE: 22.3585\n",
            "Epoch 351, Train MAE: 22.5787\n",
            "Epoch 352, Train MAE: 21.8333\n",
            "Epoch 353, Train MAE: 22.2198\n",
            "Epoch 354, Train MAE: 22.7454\n",
            "Epoch 355, Train MAE: 23.2511\n",
            "Epoch 356, Train MAE: 22.2654\n",
            "Epoch 357, Train MAE: 22.1407\n",
            "Epoch 358, Train MAE: 22.6832\n",
            "Epoch 359, Train MAE: 22.6366\n",
            "Epoch 360, Train MAE: 21.6538\n",
            "Epoch 361, Train MAE: 22.6588\n",
            "Epoch 362, Train MAE: 21.0582\n",
            "Epoch 363, Train MAE: 21.9215\n",
            "Epoch 364, Train MAE: 22.7952\n",
            "Epoch 365, Train MAE: 20.8804\n",
            "Epoch 366, Train MAE: 22.8189\n",
            "Epoch 367, Train MAE: 22.6869\n",
            "Epoch 368, Train MAE: 21.9328\n",
            "Epoch 369, Train MAE: 21.0845\n",
            "Epoch 370, Train MAE: 21.9264\n",
            "Epoch 371, Train MAE: 21.0925\n",
            "Epoch 372, Train MAE: 22.1263\n",
            "Epoch 373, Train MAE: 22.6559\n",
            "Epoch 374, Train MAE: 22.4291\n",
            "Epoch 375, Train MAE: 22.2929\n",
            "Epoch 376, Train MAE: 21.5122\n",
            "Epoch 377, Train MAE: 21.4380\n",
            "Epoch 378, Train MAE: 21.8045\n",
            "Epoch 379, Train MAE: 22.2925\n",
            "Epoch 380, Train MAE: 22.0776\n",
            "Epoch 381, Train MAE: 21.5776\n",
            "Epoch 382, Train MAE: 23.2431\n",
            "Epoch 383, Train MAE: 20.4964\n",
            "Epoch 384, Train MAE: 21.4692\n",
            "Epoch 385, Train MAE: 22.9240\n",
            "Epoch 386, Train MAE: 20.9953\n",
            "Epoch 387, Train MAE: 21.7367\n",
            "Epoch 388, Train MAE: 21.9386\n",
            "Epoch 389, Train MAE: 22.1956\n",
            "Epoch 390, Train MAE: 22.6660\n",
            "Epoch 391, Train MAE: 22.4763\n",
            "Epoch 392, Train MAE: 22.6467\n",
            "Epoch 393, Train MAE: 20.9960\n",
            "Epoch 394, Train MAE: 21.0605\n",
            "Epoch 395, Train MAE: 22.1952\n",
            "Epoch 396, Train MAE: 22.5169\n",
            "Epoch 397, Train MAE: 21.8519\n",
            "Epoch 398, Train MAE: 21.6589\n",
            "Epoch 399, Train MAE: 22.2656\n",
            "Epoch 400, Train MAE: 21.5380\n",
            "Epoch 401, Train MAE: 22.3696\n",
            "Epoch 402, Train MAE: 22.2768\n",
            "Epoch 403, Train MAE: 22.8915\n",
            "Epoch 404, Train MAE: 21.2626\n",
            "Epoch 405, Train MAE: 22.2892\n",
            "Epoch 406, Train MAE: 22.9400\n",
            "Epoch 407, Train MAE: 21.9484\n",
            "Epoch 408, Train MAE: 21.9202\n",
            "Epoch 409, Train MAE: 21.8257\n",
            "Epoch 410, Train MAE: 21.6865\n",
            "Epoch 411, Train MAE: 22.5726\n",
            "Epoch 412, Train MAE: 22.3477\n",
            "Epoch 413, Train MAE: 22.2430\n",
            "Epoch 414, Train MAE: 21.6182\n",
            "Epoch 415, Train MAE: 20.6662\n",
            "Epoch 416, Train MAE: 22.2599\n",
            "Epoch 417, Train MAE: 21.2791\n",
            "Epoch 418, Train MAE: 21.8771\n",
            "Epoch 419, Train MAE: 22.3787\n",
            "Epoch 420, Train MAE: 20.4471\n",
            "Epoch 421, Train MAE: 22.7457\n",
            "Epoch 422, Train MAE: 22.5612\n",
            "Epoch 423, Train MAE: 22.6898\n",
            "Epoch 424, Train MAE: 22.5179\n",
            "Epoch 425, Train MAE: 21.9133\n",
            "Epoch 426, Train MAE: 22.8103\n",
            "Epoch 427, Train MAE: 22.2328\n",
            "Epoch 428, Train MAE: 21.7959\n",
            "Epoch 429, Train MAE: 21.9182\n",
            "Epoch 430, Train MAE: 20.8398\n",
            "Epoch 431, Train MAE: 21.3101\n",
            "Epoch 432, Train MAE: 21.3447\n",
            "Epoch 433, Train MAE: 22.3168\n",
            "Epoch 434, Train MAE: 23.6530\n",
            "Epoch 435, Train MAE: 21.5255\n",
            "Epoch 436, Train MAE: 22.5572\n",
            "Epoch 437, Train MAE: 20.8888\n",
            "Epoch 438, Train MAE: 22.0757\n",
            "Epoch 439, Train MAE: 22.3491\n",
            "Epoch 440, Train MAE: 22.1178\n",
            "Epoch 441, Train MAE: 22.5048\n",
            "Epoch 442, Train MAE: 22.2030\n",
            "Epoch 443, Train MAE: 21.9665\n",
            "Epoch 444, Train MAE: 21.3564\n",
            "Epoch 445, Train MAE: 22.1798\n",
            "Epoch 446, Train MAE: 22.3541\n",
            "Epoch 447, Train MAE: 22.1987\n",
            "Epoch 448, Train MAE: 21.8468\n",
            "Epoch 449, Train MAE: 22.0883\n",
            "Epoch 450, Train MAE: 22.2114\n",
            "Epoch 451, Train MAE: 22.2517\n",
            "Epoch 452, Train MAE: 21.5125\n",
            "Epoch 453, Train MAE: 23.2034\n",
            "Epoch 454, Train MAE: 21.9045\n",
            "Epoch 455, Train MAE: 22.0201\n",
            "Epoch 456, Train MAE: 21.2198\n",
            "Epoch 457, Train MAE: 22.1589\n",
            "Epoch 458, Train MAE: 22.2449\n",
            "Epoch 459, Train MAE: 22.3567\n",
            "Epoch 460, Train MAE: 21.4934\n",
            "Epoch 461, Train MAE: 23.1266\n",
            "Epoch 462, Train MAE: 21.3374\n",
            "Epoch 463, Train MAE: 21.5081\n",
            "Epoch 464, Train MAE: 22.2235\n",
            "Epoch 465, Train MAE: 21.5152\n",
            "Epoch 466, Train MAE: 22.9192\n",
            "Epoch 467, Train MAE: 22.5687\n",
            "Epoch 468, Train MAE: 22.2905\n",
            "Epoch 469, Train MAE: 21.7181\n",
            "Epoch 470, Train MAE: 21.3368\n",
            "Epoch 471, Train MAE: 22.7708\n",
            "Epoch 472, Train MAE: 21.9789\n",
            "Epoch 473, Train MAE: 22.6762\n",
            "Epoch 474, Train MAE: 22.2373\n",
            "Epoch 475, Train MAE: 21.9170\n",
            "Epoch 476, Train MAE: 21.9922\n",
            "Epoch 477, Train MAE: 22.2591\n",
            "Epoch 478, Train MAE: 21.7023\n",
            "Epoch 479, Train MAE: 22.7406\n",
            "Epoch 480, Train MAE: 21.6123\n",
            "Epoch 481, Train MAE: 20.7218\n",
            "Epoch 482, Train MAE: 21.2398\n",
            "Epoch 483, Train MAE: 21.8701\n",
            "Epoch 484, Train MAE: 22.0620\n",
            "Epoch 485, Train MAE: 21.6764\n",
            "Epoch 486, Train MAE: 22.2876\n",
            "Epoch 487, Train MAE: 21.5332\n",
            "Epoch 488, Train MAE: 21.9874\n",
            "Epoch 489, Train MAE: 22.0715\n",
            "Epoch 490, Train MAE: 21.8982\n",
            "Epoch 491, Train MAE: 22.4742\n",
            "Epoch 492, Train MAE: 22.0826\n",
            "Epoch 493, Train MAE: 21.6795\n",
            "Epoch 494, Train MAE: 23.0357\n",
            "Epoch 495, Train MAE: 21.5331\n",
            "Epoch 496, Train MAE: 21.8392\n",
            "Epoch 497, Train MAE: 22.9821\n",
            "Epoch 498, Train MAE: 21.6664\n",
            "Epoch 499, Train MAE: 21.7463\n",
            "Epoch 500, Train MAE: 21.9284\n",
            "Epoch 501, Train MAE: 22.6933\n",
            "Epoch 502, Train MAE: 22.9620\n",
            "Epoch 503, Train MAE: 22.1245\n",
            "Epoch 504, Train MAE: 22.6528\n",
            "Epoch 505, Train MAE: 21.8538\n",
            "Epoch 506, Train MAE: 22.1778\n",
            "Epoch 507, Train MAE: 21.4588\n",
            "Epoch 508, Train MAE: 22.4229\n",
            "Epoch 509, Train MAE: 21.4461\n",
            "Epoch 510, Train MAE: 22.4275\n",
            "Epoch 511, Train MAE: 21.5456\n",
            "Epoch 512, Train MAE: 21.5956\n",
            "Epoch 513, Train MAE: 22.3662\n",
            "Epoch 514, Train MAE: 21.0226\n",
            "Epoch 515, Train MAE: 20.9172\n",
            "Epoch 516, Train MAE: 22.8704\n",
            "Epoch 517, Train MAE: 21.9025\n",
            "Epoch 518, Train MAE: 21.4746\n",
            "Epoch 519, Train MAE: 22.7174\n",
            "Epoch 520, Train MAE: 20.7794\n",
            "Epoch 521, Train MAE: 22.6841\n",
            "Epoch 522, Train MAE: 22.1253\n",
            "Epoch 523, Train MAE: 23.1862\n",
            "Epoch 524, Train MAE: 22.4793\n",
            "Epoch 525, Train MAE: 22.2024\n",
            "Epoch 526, Train MAE: 21.3564\n",
            "Epoch 527, Train MAE: 20.6342\n",
            "Epoch 528, Train MAE: 23.3140\n",
            "Epoch 529, Train MAE: 22.7636\n",
            "Epoch 530, Train MAE: 21.3973\n",
            "Epoch 531, Train MAE: 22.1355\n",
            "Epoch 532, Train MAE: 21.6822\n",
            "Epoch 533, Train MAE: 21.4702\n",
            "Epoch 534, Train MAE: 21.4946\n",
            "Epoch 535, Train MAE: 22.1610\n",
            "Epoch 536, Train MAE: 22.5277\n",
            "Epoch 537, Train MAE: 21.5202\n",
            "Epoch 538, Train MAE: 21.5482\n",
            "Epoch 539, Train MAE: 22.1591\n",
            "Epoch 540, Train MAE: 22.9774\n",
            "Epoch 541, Train MAE: 22.8433\n",
            "Epoch 542, Train MAE: 23.1161\n",
            "Epoch 543, Train MAE: 22.7723\n",
            "Epoch 544, Train MAE: 21.2886\n",
            "Epoch 545, Train MAE: 21.6180\n",
            "Epoch 546, Train MAE: 24.0436\n",
            "Epoch 547, Train MAE: 21.8632\n",
            "Epoch 548, Train MAE: 22.2391\n",
            "Epoch 549, Train MAE: 22.9745\n",
            "Epoch 550, Train MAE: 20.8374\n",
            "Epoch 551, Train MAE: 22.6350\n",
            "Epoch 552, Train MAE: 21.7306\n",
            "Epoch 553, Train MAE: 21.5944\n",
            "Epoch 554, Train MAE: 22.0337\n",
            "Epoch 555, Train MAE: 22.9828\n",
            "Epoch 556, Train MAE: 22.4979\n",
            "Epoch 557, Train MAE: 20.6363\n",
            "Epoch 558, Train MAE: 22.2497\n",
            "Epoch 559, Train MAE: 22.2786\n",
            "Epoch 560, Train MAE: 22.6832\n",
            "Epoch 561, Train MAE: 22.7983\n",
            "Epoch 562, Train MAE: 21.5778\n",
            "Epoch 563, Train MAE: 21.7784\n",
            "Epoch 564, Train MAE: 22.2518\n",
            "Epoch 565, Train MAE: 22.1977\n",
            "Epoch 566, Train MAE: 22.4427\n",
            "Epoch 567, Train MAE: 22.6158\n",
            "Epoch 568, Train MAE: 22.0820\n",
            "Epoch 569, Train MAE: 21.7263\n",
            "Epoch 570, Train MAE: 22.3062\n",
            "Epoch 571, Train MAE: 21.4154\n",
            "Epoch 572, Train MAE: 22.1999\n",
            "Epoch 573, Train MAE: 22.2815\n",
            "Epoch 574, Train MAE: 22.5753\n",
            "Epoch 575, Train MAE: 22.6201\n",
            "Epoch 576, Train MAE: 22.0430\n",
            "Epoch 577, Train MAE: 21.1783\n",
            "Epoch 578, Train MAE: 22.8929\n",
            "Epoch 579, Train MAE: 22.4648\n",
            "Epoch 580, Train MAE: 21.8096\n",
            "Epoch 581, Train MAE: 21.8724\n",
            "Epoch 582, Train MAE: 20.9463\n",
            "Epoch 583, Train MAE: 22.3598\n",
            "Epoch 584, Train MAE: 22.3676\n",
            "Epoch 585, Train MAE: 21.7632\n",
            "Epoch 586, Train MAE: 22.3396\n",
            "Epoch 587, Train MAE: 22.5655\n",
            "Epoch 588, Train MAE: 22.0450\n",
            "Epoch 589, Train MAE: 22.4859\n",
            "Epoch 590, Train MAE: 22.2047\n",
            "Epoch 591, Train MAE: 22.7932\n",
            "Epoch 592, Train MAE: 22.5273\n",
            "Epoch 593, Train MAE: 21.5754\n",
            "Epoch 594, Train MAE: 22.0633\n",
            "Epoch 595, Train MAE: 22.5461\n",
            "Epoch 596, Train MAE: 21.5957\n",
            "Epoch 597, Train MAE: 21.8050\n",
            "Epoch 598, Train MAE: 21.8076\n",
            "Epoch 599, Train MAE: 23.2383\n",
            "Epoch 600, Train MAE: 21.7798\n",
            "Epoch 601, Train MAE: 22.7519\n",
            "Epoch 602, Train MAE: 21.9303\n",
            "Epoch 603, Train MAE: 21.8038\n",
            "Epoch 604, Train MAE: 21.4986\n",
            "Epoch 605, Train MAE: 22.5046\n",
            "Epoch 606, Train MAE: 22.5842\n",
            "Epoch 607, Train MAE: 21.5514\n",
            "Epoch 608, Train MAE: 21.8453\n",
            "Epoch 609, Train MAE: 21.8270\n",
            "Epoch 610, Train MAE: 22.3901\n",
            "Epoch 611, Train MAE: 23.1894\n",
            "Epoch 612, Train MAE: 22.4153\n",
            "Epoch 613, Train MAE: 21.8812\n",
            "Epoch 614, Train MAE: 20.8396\n",
            "Epoch 615, Train MAE: 21.4314\n",
            "Epoch 616, Train MAE: 22.2199\n",
            "Epoch 617, Train MAE: 21.5470\n",
            "Epoch 618, Train MAE: 22.0941\n",
            "Epoch 619, Train MAE: 21.0654\n",
            "Epoch 620, Train MAE: 21.8074\n",
            "Epoch 621, Train MAE: 22.1827\n",
            "Epoch 622, Train MAE: 22.3189\n",
            "Epoch 623, Train MAE: 21.4996\n",
            "Epoch 624, Train MAE: 22.4854\n",
            "Epoch 625, Train MAE: 21.7248\n",
            "Epoch 626, Train MAE: 21.8477\n",
            "Epoch 627, Train MAE: 21.5095\n",
            "Epoch 628, Train MAE: 21.8817\n",
            "Epoch 629, Train MAE: 20.7524\n",
            "Epoch 630, Train MAE: 22.6189\n",
            "Epoch 631, Train MAE: 22.0746\n",
            "Epoch 632, Train MAE: 22.8945\n",
            "Epoch 633, Train MAE: 22.2436\n",
            "Epoch 634, Train MAE: 22.1576\n",
            "Epoch 635, Train MAE: 21.3070\n",
            "Epoch 636, Train MAE: 21.6822\n",
            "Epoch 637, Train MAE: 23.1609\n",
            "Epoch 638, Train MAE: 22.5343\n",
            "Epoch 639, Train MAE: 23.2105\n",
            "Epoch 640, Train MAE: 22.7820\n",
            "Epoch 641, Train MAE: 22.7261\n",
            "Epoch 642, Train MAE: 22.1936\n",
            "Epoch 643, Train MAE: 21.0283\n",
            "Epoch 644, Train MAE: 21.4351\n",
            "Epoch 645, Train MAE: 21.3969\n",
            "Epoch 646, Train MAE: 22.1414\n",
            "Epoch 647, Train MAE: 22.0051\n",
            "Epoch 648, Train MAE: 23.0594\n",
            "Epoch 649, Train MAE: 22.6706\n",
            "Epoch 650, Train MAE: 22.9101\n",
            "Epoch 651, Train MAE: 21.5407\n",
            "Epoch 652, Train MAE: 23.0793\n",
            "Epoch 653, Train MAE: 22.6104\n",
            "Epoch 654, Train MAE: 20.7623\n",
            "Epoch 655, Train MAE: 21.5605\n",
            "Epoch 656, Train MAE: 22.0808\n",
            "Epoch 657, Train MAE: 22.3257\n",
            "Epoch 658, Train MAE: 22.5765\n",
            "Epoch 659, Train MAE: 22.3232\n",
            "Epoch 660, Train MAE: 22.7080\n",
            "Epoch 661, Train MAE: 22.8147\n",
            "Epoch 662, Train MAE: 22.6172\n",
            "Epoch 663, Train MAE: 22.2169\n",
            "Epoch 664, Train MAE: 22.6640\n",
            "Epoch 665, Train MAE: 21.8346\n",
            "Epoch 666, Train MAE: 21.3804\n",
            "Epoch 667, Train MAE: 22.1886\n",
            "Epoch 668, Train MAE: 22.5607\n",
            "Epoch 669, Train MAE: 21.7545\n",
            "Epoch 670, Train MAE: 21.4431\n",
            "Epoch 671, Train MAE: 22.1739\n",
            "Epoch 672, Train MAE: 22.1796\n",
            "Epoch 673, Train MAE: 21.9400\n",
            "Epoch 674, Train MAE: 20.9610\n",
            "Epoch 675, Train MAE: 21.6904\n",
            "Epoch 676, Train MAE: 22.3796\n",
            "Epoch 677, Train MAE: 22.2046\n",
            "Epoch 678, Train MAE: 22.0478\n",
            "Epoch 679, Train MAE: 22.7671\n",
            "Epoch 680, Train MAE: 22.5804\n",
            "Epoch 681, Train MAE: 22.5669\n",
            "Epoch 682, Train MAE: 22.5322\n",
            "Epoch 683, Train MAE: 22.5008\n",
            "Epoch 684, Train MAE: 21.4029\n",
            "Epoch 685, Train MAE: 22.1561\n",
            "Epoch 686, Train MAE: 22.1503\n",
            "Epoch 687, Train MAE: 20.6563\n",
            "Epoch 688, Train MAE: 22.4461\n",
            "Epoch 689, Train MAE: 22.7133\n",
            "Epoch 690, Train MAE: 21.5389\n",
            "Epoch 691, Train MAE: 21.8302\n",
            "Epoch 692, Train MAE: 22.0459\n",
            "Epoch 693, Train MAE: 21.2114\n",
            "Epoch 694, Train MAE: 21.7130\n",
            "Epoch 695, Train MAE: 21.4161\n",
            "Epoch 696, Train MAE: 21.4117\n",
            "Epoch 697, Train MAE: 22.3827\n",
            "Epoch 698, Train MAE: 22.5674\n",
            "Epoch 699, Train MAE: 21.8181\n",
            "Epoch 700, Train MAE: 21.6651\n",
            "Epoch 701, Train MAE: 22.8356\n",
            "Epoch 702, Train MAE: 22.3556\n",
            "Epoch 703, Train MAE: 21.1844\n",
            "Epoch 704, Train MAE: 21.9102\n",
            "Epoch 705, Train MAE: 22.3380\n",
            "Epoch 706, Train MAE: 22.0389\n",
            "Epoch 707, Train MAE: 20.6904\n",
            "Epoch 708, Train MAE: 20.9327\n",
            "Epoch 709, Train MAE: 23.2564\n",
            "Epoch 710, Train MAE: 22.6911\n",
            "Epoch 711, Train MAE: 21.0059\n",
            "Epoch 712, Train MAE: 21.7541\n",
            "Epoch 713, Train MAE: 22.4846\n",
            "Epoch 714, Train MAE: 20.8530\n",
            "Epoch 715, Train MAE: 21.2064\n",
            "Epoch 716, Train MAE: 22.4976\n",
            "Epoch 717, Train MAE: 22.6839\n",
            "Epoch 718, Train MAE: 21.8029\n",
            "Epoch 719, Train MAE: 21.5526\n",
            "Epoch 720, Train MAE: 22.6501\n",
            "Epoch 721, Train MAE: 22.2979\n",
            "Epoch 722, Train MAE: 22.5297\n",
            "Epoch 723, Train MAE: 21.8571\n",
            "Epoch 724, Train MAE: 22.3682\n",
            "Epoch 725, Train MAE: 22.7560\n",
            "Epoch 726, Train MAE: 22.3189\n",
            "Epoch 727, Train MAE: 22.3308\n",
            "Epoch 728, Train MAE: 22.2939\n",
            "Epoch 729, Train MAE: 23.1171\n",
            "Epoch 730, Train MAE: 22.5543\n",
            "Epoch 731, Train MAE: 21.9772\n",
            "Epoch 732, Train MAE: 22.5695\n",
            "Epoch 733, Train MAE: 22.2668\n",
            "Epoch 734, Train MAE: 21.6681\n",
            "Epoch 735, Train MAE: 21.5094\n",
            "Epoch 736, Train MAE: 21.9172\n",
            "Epoch 737, Train MAE: 21.5847\n",
            "Epoch 738, Train MAE: 22.0070\n",
            "Epoch 739, Train MAE: 22.2558\n",
            "Epoch 740, Train MAE: 21.7457\n",
            "Epoch 741, Train MAE: 22.4209\n",
            "Epoch 742, Train MAE: 21.6319\n",
            "Epoch 743, Train MAE: 21.5543\n",
            "Epoch 744, Train MAE: 22.0845\n",
            "Epoch 745, Train MAE: 22.4178\n",
            "Epoch 746, Train MAE: 21.6368\n",
            "Epoch 747, Train MAE: 22.8141\n",
            "Epoch 748, Train MAE: 21.9946\n",
            "Epoch 749, Train MAE: 23.0343\n",
            "Epoch 750, Train MAE: 22.7796\n",
            "Epoch 751, Train MAE: 22.6530\n",
            "Epoch 752, Train MAE: 22.2618\n",
            "Epoch 753, Train MAE: 21.9576\n",
            "Epoch 754, Train MAE: 21.7369\n",
            "Epoch 755, Train MAE: 22.0768\n",
            "Epoch 756, Train MAE: 22.0399\n",
            "Epoch 757, Train MAE: 21.8928\n",
            "Epoch 758, Train MAE: 22.7578\n",
            "Epoch 759, Train MAE: 21.2577\n",
            "Epoch 760, Train MAE: 22.3964\n",
            "Epoch 761, Train MAE: 22.7119\n",
            "Epoch 762, Train MAE: 22.0416\n",
            "Epoch 763, Train MAE: 21.8959\n",
            "Epoch 764, Train MAE: 23.4945\n",
            "Epoch 765, Train MAE: 21.7991\n",
            "Epoch 766, Train MAE: 22.5976\n",
            "Epoch 767, Train MAE: 21.7165\n",
            "Epoch 768, Train MAE: 22.5789\n",
            "Epoch 769, Train MAE: 21.9055\n",
            "Epoch 770, Train MAE: 22.4167\n",
            "Epoch 771, Train MAE: 22.3130\n",
            "Epoch 772, Train MAE: 22.9028\n",
            "Epoch 773, Train MAE: 21.9393\n",
            "Epoch 774, Train MAE: 22.3153\n",
            "Epoch 775, Train MAE: 22.9590\n",
            "Epoch 776, Train MAE: 22.5540\n",
            "Epoch 777, Train MAE: 22.2678\n",
            "Epoch 778, Train MAE: 21.6055\n",
            "Epoch 779, Train MAE: 22.7840\n",
            "Epoch 780, Train MAE: 21.4854\n",
            "Epoch 781, Train MAE: 21.9678\n",
            "Epoch 782, Train MAE: 21.8958\n",
            "Epoch 783, Train MAE: 21.3717\n",
            "Epoch 784, Train MAE: 22.7943\n",
            "Epoch 785, Train MAE: 21.5726\n",
            "Epoch 786, Train MAE: 22.3380\n",
            "Epoch 787, Train MAE: 22.5038\n",
            "Epoch 788, Train MAE: 22.9693\n",
            "Epoch 789, Train MAE: 21.5481\n",
            "Epoch 790, Train MAE: 22.0728\n",
            "Epoch 791, Train MAE: 22.8007\n",
            "Epoch 792, Train MAE: 21.7250\n",
            "Epoch 793, Train MAE: 22.3787\n",
            "Epoch 794, Train MAE: 21.6636\n",
            "Epoch 795, Train MAE: 21.3888\n",
            "Epoch 796, Train MAE: 22.7738\n",
            "Epoch 797, Train MAE: 21.7869\n",
            "Epoch 798, Train MAE: 21.8571\n",
            "Epoch 799, Train MAE: 21.8419\n",
            "Epoch 800, Train MAE: 21.8334\n",
            "Epoch 801, Train MAE: 20.9389\n",
            "Epoch 802, Train MAE: 22.3364\n",
            "Epoch 803, Train MAE: 21.4183\n",
            "Epoch 804, Train MAE: 22.9003\n",
            "Epoch 805, Train MAE: 22.4281\n",
            "Epoch 806, Train MAE: 22.0455\n",
            "Epoch 807, Train MAE: 22.9280\n",
            "Epoch 808, Train MAE: 22.1998\n",
            "Epoch 809, Train MAE: 21.3361\n",
            "Epoch 810, Train MAE: 21.6597\n",
            "Epoch 811, Train MAE: 22.5179\n",
            "Epoch 812, Train MAE: 21.5551\n",
            "Epoch 813, Train MAE: 22.5126\n",
            "Epoch 814, Train MAE: 22.5758\n",
            "Epoch 815, Train MAE: 22.5049\n",
            "Epoch 816, Train MAE: 21.4036\n",
            "Epoch 817, Train MAE: 22.2678\n",
            "Epoch 818, Train MAE: 22.6956\n",
            "Epoch 819, Train MAE: 22.2492\n",
            "Epoch 820, Train MAE: 23.0914\n",
            "Epoch 821, Train MAE: 22.8324\n",
            "Epoch 822, Train MAE: 22.2117\n",
            "Epoch 823, Train MAE: 22.0521\n",
            "Epoch 824, Train MAE: 22.7929\n",
            "Epoch 825, Train MAE: 21.4621\n",
            "Epoch 826, Train MAE: 23.3255\n",
            "Epoch 827, Train MAE: 22.2238\n",
            "Epoch 828, Train MAE: 22.7205\n",
            "Epoch 829, Train MAE: 21.6993\n",
            "Epoch 830, Train MAE: 22.1036\n",
            "Epoch 831, Train MAE: 21.0616\n",
            "Epoch 832, Train MAE: 22.0889\n",
            "Epoch 833, Train MAE: 22.9241\n",
            "Epoch 834, Train MAE: 21.8477\n",
            "Epoch 835, Train MAE: 22.7697\n",
            "Epoch 836, Train MAE: 22.1441\n",
            "Epoch 837, Train MAE: 21.7902\n",
            "Epoch 838, Train MAE: 21.9985\n",
            "Epoch 839, Train MAE: 22.2314\n",
            "Epoch 840, Train MAE: 22.4511\n",
            "Epoch 841, Train MAE: 22.5673\n",
            "Epoch 842, Train MAE: 22.2643\n",
            "Epoch 843, Train MAE: 23.1912\n",
            "Epoch 844, Train MAE: 21.9922\n",
            "Epoch 845, Train MAE: 22.5544\n",
            "Epoch 846, Train MAE: 21.7335\n",
            "Epoch 847, Train MAE: 22.6688\n",
            "Epoch 848, Train MAE: 22.4192\n",
            "Epoch 849, Train MAE: 21.9111\n",
            "Epoch 850, Train MAE: 21.6338\n",
            "Epoch 851, Train MAE: 21.5271\n",
            "Epoch 852, Train MAE: 21.5966\n",
            "Epoch 853, Train MAE: 22.6591\n",
            "Epoch 854, Train MAE: 22.2049\n",
            "Epoch 855, Train MAE: 21.6605\n",
            "Epoch 856, Train MAE: 22.0722\n",
            "Epoch 857, Train MAE: 22.2937\n",
            "Epoch 858, Train MAE: 22.8652\n",
            "Epoch 859, Train MAE: 21.9095\n",
            "Epoch 860, Train MAE: 21.8332\n",
            "Epoch 861, Train MAE: 23.1889\n",
            "Epoch 862, Train MAE: 22.9481\n",
            "Epoch 863, Train MAE: 21.6547\n",
            "Epoch 864, Train MAE: 22.1220\n",
            "Epoch 865, Train MAE: 22.3423\n",
            "Epoch 866, Train MAE: 21.8980\n",
            "Epoch 867, Train MAE: 21.3348\n",
            "Epoch 868, Train MAE: 22.1683\n",
            "Epoch 869, Train MAE: 21.0352\n",
            "Epoch 870, Train MAE: 21.1860\n",
            "Epoch 871, Train MAE: 21.8055\n",
            "Epoch 872, Train MAE: 21.9591\n",
            "Epoch 873, Train MAE: 21.3875\n",
            "Epoch 874, Train MAE: 22.8959\n",
            "Epoch 875, Train MAE: 22.3858\n",
            "Epoch 876, Train MAE: 21.8216\n",
            "Epoch 877, Train MAE: 22.1339\n",
            "Epoch 878, Train MAE: 22.1609\n",
            "Epoch 879, Train MAE: 22.2705\n",
            "Epoch 880, Train MAE: 22.2511\n",
            "Epoch 881, Train MAE: 21.7542\n",
            "Epoch 882, Train MAE: 22.5978\n",
            "Epoch 883, Train MAE: 21.8437\n",
            "Epoch 884, Train MAE: 21.8204\n",
            "Epoch 885, Train MAE: 21.5305\n",
            "Epoch 886, Train MAE: 22.3313\n",
            "Epoch 887, Train MAE: 22.1575\n",
            "Epoch 888, Train MAE: 21.3366\n",
            "Epoch 889, Train MAE: 22.2269\n",
            "Epoch 890, Train MAE: 22.7869\n",
            "Epoch 891, Train MAE: 21.5063\n",
            "Epoch 892, Train MAE: 21.4289\n",
            "Epoch 893, Train MAE: 22.3451\n",
            "Epoch 894, Train MAE: 22.8229\n",
            "Epoch 895, Train MAE: 21.8709\n",
            "Epoch 896, Train MAE: 22.8756\n",
            "Epoch 897, Train MAE: 22.0217\n",
            "Epoch 898, Train MAE: 22.5318\n",
            "Epoch 899, Train MAE: 22.7376\n",
            "Epoch 900, Train MAE: 22.0855\n",
            "Epoch 901, Train MAE: 21.5733\n",
            "Epoch 902, Train MAE: 21.2767\n",
            "Epoch 903, Train MAE: 21.5521\n",
            "Epoch 904, Train MAE: 21.5350\n",
            "Epoch 905, Train MAE: 23.3213\n",
            "Epoch 906, Train MAE: 22.5139\n",
            "Epoch 907, Train MAE: 21.0053\n",
            "Epoch 908, Train MAE: 21.1286\n",
            "Epoch 909, Train MAE: 22.4215\n",
            "Epoch 910, Train MAE: 22.1077\n",
            "Epoch 911, Train MAE: 22.0868\n",
            "Epoch 912, Train MAE: 21.7623\n",
            "Epoch 913, Train MAE: 23.3186\n",
            "Epoch 914, Train MAE: 21.3116\n",
            "Epoch 915, Train MAE: 21.5516\n",
            "Epoch 916, Train MAE: 21.6049\n",
            "Epoch 917, Train MAE: 21.6099\n",
            "Epoch 918, Train MAE: 21.5340\n",
            "Epoch 919, Train MAE: 21.9367\n",
            "Epoch 920, Train MAE: 22.0829\n",
            "Epoch 921, Train MAE: 21.7143\n",
            "Epoch 922, Train MAE: 21.1730\n",
            "Epoch 923, Train MAE: 21.7455\n",
            "Epoch 924, Train MAE: 22.3329\n",
            "Epoch 925, Train MAE: 22.6527\n",
            "Epoch 926, Train MAE: 21.1242\n",
            "Epoch 927, Train MAE: 21.9894\n",
            "Epoch 928, Train MAE: 20.5315\n",
            "Epoch 929, Train MAE: 22.8892\n",
            "Epoch 930, Train MAE: 21.4988\n",
            "Epoch 931, Train MAE: 22.2634\n",
            "Epoch 932, Train MAE: 21.9720\n",
            "Epoch 933, Train MAE: 21.9866\n",
            "Epoch 934, Train MAE: 21.5299\n",
            "Epoch 935, Train MAE: 22.4482\n",
            "Epoch 936, Train MAE: 21.7828\n",
            "Epoch 937, Train MAE: 21.9976\n",
            "Epoch 938, Train MAE: 22.4147\n",
            "Epoch 939, Train MAE: 22.2835\n",
            "Epoch 940, Train MAE: 21.8623\n",
            "Epoch 941, Train MAE: 22.0443\n",
            "Epoch 942, Train MAE: 22.2158\n",
            "Epoch 943, Train MAE: 22.9541\n",
            "Epoch 944, Train MAE: 22.6005\n",
            "Epoch 945, Train MAE: 22.3932\n",
            "Epoch 946, Train MAE: 21.6872\n",
            "Epoch 947, Train MAE: 21.3440\n",
            "Epoch 948, Train MAE: 20.9816\n",
            "Epoch 949, Train MAE: 22.1393\n",
            "Epoch 950, Train MAE: 22.2426\n",
            "Epoch 951, Train MAE: 22.2481\n",
            "Epoch 952, Train MAE: 23.3155\n",
            "Epoch 953, Train MAE: 21.9360\n",
            "Epoch 954, Train MAE: 22.2390\n",
            "Epoch 955, Train MAE: 22.3131\n",
            "Epoch 956, Train MAE: 21.0950\n",
            "Epoch 957, Train MAE: 21.9449\n",
            "Epoch 958, Train MAE: 23.3468\n",
            "Epoch 959, Train MAE: 22.1014\n",
            "Epoch 960, Train MAE: 23.3619\n",
            "Epoch 961, Train MAE: 22.2977\n",
            "Epoch 962, Train MAE: 21.5314\n",
            "Epoch 963, Train MAE: 22.5245\n",
            "Epoch 964, Train MAE: 21.4560\n",
            "Epoch 965, Train MAE: 21.8800\n",
            "Epoch 966, Train MAE: 21.9060\n",
            "Epoch 967, Train MAE: 22.0050\n",
            "Epoch 968, Train MAE: 21.6265\n",
            "Epoch 969, Train MAE: 22.1786\n",
            "Epoch 970, Train MAE: 23.3165\n",
            "Epoch 971, Train MAE: 22.1047\n",
            "Epoch 972, Train MAE: 21.9839\n",
            "Epoch 973, Train MAE: 21.6475\n",
            "Epoch 974, Train MAE: 22.4463\n",
            "Epoch 975, Train MAE: 22.9057\n",
            "Epoch 976, Train MAE: 22.0504\n",
            "Epoch 977, Train MAE: 23.4597\n",
            "Epoch 978, Train MAE: 21.5588\n",
            "Epoch 979, Train MAE: 22.6621\n",
            "Epoch 980, Train MAE: 21.4978\n",
            "Epoch 981, Train MAE: 21.5485\n",
            "Epoch 982, Train MAE: 22.4271\n",
            "Epoch 983, Train MAE: 21.1106\n",
            "Epoch 984, Train MAE: 22.0642\n",
            "Epoch 985, Train MAE: 21.8102\n",
            "Epoch 986, Train MAE: 22.3924\n",
            "Epoch 987, Train MAE: 21.9582\n",
            "Epoch 988, Train MAE: 23.1544\n",
            "Epoch 989, Train MAE: 21.9075\n",
            "Epoch 990, Train MAE: 22.1937\n",
            "Epoch 991, Train MAE: 21.8793\n",
            "Epoch 992, Train MAE: 21.7074\n",
            "Epoch 993, Train MAE: 21.1415\n",
            "Epoch 994, Train MAE: 21.8192\n",
            "Epoch 995, Train MAE: 20.6179\n",
            "Epoch 996, Train MAE: 22.7763\n",
            "Epoch 997, Train MAE: 21.5744\n",
            "Epoch 998, Train MAE: 22.0136\n",
            "Epoch 999, Train MAE: 22.1391\n",
            "Epoch 1000, Train MAE: 22.1690\n",
            "Best Train MAE: 15.5329075\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Use TemporalTransformer on aggregated features to predict return_on_asset\n",
        "\n",
        "\n",
        "# Prepare a dataset for tabular (aggregated) features\n",
        "class TabularAggDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.X[idx], self.y[idx]\n",
        "        else:\n",
        "            return self.X[idx]\n",
        "\n",
        "# Use only the features (no target) for test set\n",
        "X_test = test_agg_imputed.values.astype(np.float32)\n",
        "test_ds = TabularAggDataset(test_agg_imputed)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# For training, use X_train_imputed and y_train\n",
        "X_train = X_train_imputed.values.astype(np.float32)\n",
        "y_train_tensor = y_train.values.astype(np.float32)\n",
        "train_ds = TabularAggDataset(X_train_imputed, y_train)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define a simple transformer for tabular data\n",
        "import torch.nn as nn\n",
        "\n",
        "class TabularTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(d_model, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, features)\n",
        "        x = self.input_proj(x).unsqueeze(1)  # (batch, 1, d_model)\n",
        "        x = self.transformer(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TabularTransformer(input_dim=X_train.shape[1]).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "best_mae = float('inf')\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb).squeeze()\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation on train set (for simplicity)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            pred = model(xb).squeeze().cpu().numpy()\n",
        "            preds.append(pred)\n",
        "        preds = np.concatenate([p.reshape(-1) for p in preds])\n",
        "        mae = np.mean(np.abs(preds - y_train_tensor))\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "    print(f\"Epoch {epoch+1}, Train MAE: {mae:.4f}\")\n",
        "\n",
        "print(\"Best Train MAE:\", best_mae)\n",
        "\n",
        "# Predict on test set\n",
        "model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).squeeze().cpu().numpy()\n",
        "        test_preds.append(pred)\n",
        "test_preds = np.concatenate([p.reshape(-1) for p in test_preds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2de6e0aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n",
            "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest on Transformer Embeddings CV MAE: [12.85038835 13.67769608 12.77223039 11.49504902 13.79617647]\n",
            "Mean MAE: 12.918308062059774\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Step 1: Get transformer embeddings for train and test sets\n",
        "# We'll use the output of the penultimate layer (before the regression head) as features\n",
        "\n",
        "def get_transformer_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            # Forward pass up to the transformer encoder\n",
        "            x_proj = model.input_proj(xb)\n",
        "            x_enc = model.transformer(x_proj)\n",
        "            # Flatten for each sample\n",
        "            emb = x_enc.reshape(x_enc.shape[0], -1).cpu().numpy()\n",
        "            embeddings.append(emb)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Prepare train sequence dataset and loader\n",
        "seq_len = 32  # Use the same as in previous cells\n",
        "feature_cols = [c for c in train_df.columns if c.startswith('indicator_')]\n",
        "train_seq_ds = AssetSequenceDataset(train_df, target_col='return_on_asset', seq_len=seq_len, feature_cols=feature_cols)\n",
        "train_seq_loader = DataLoader(train_seq_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# Prepare test sequence dataset and loader\n",
        "test_seq_ds = AssetSequenceDataset(test_df, target_col=None, seq_len=seq_len, feature_cols=feature_cols)\n",
        "test_seq_loader = DataLoader(test_seq_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define and (optionally) train the temporal_model before using it\n",
        "# Assume input_dim is the number of features in your sequence data\n",
        "input_dim = len(feature_cols)\n",
        "temporal_model = TemporalTransformer(input_dim=input_dim, d_model=64, nhead=4, num_layers=2, seq_len=seq_len).to(device)\n",
        "\n",
        "# (Optional) You may want to train the model here before extracting embeddings\n",
        "# For demonstration, we'll just use the randomly initialized model\n",
        "\n",
        "# Get embeddings\n",
        "train_embeddings = get_transformer_embeddings(temporal_model, train_seq_loader, device)\n",
        "test_embeddings = get_transformer_embeddings(temporal_model, test_seq_loader, device)\n",
        "\n",
        "# Step 2: Fit RandomForest on transformer embeddings\n",
        "\n",
        "# Use y_train as target (already aligned with train_seq_ds)\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(train_embeddings, y_train.values)\n",
        "\n",
        "# Cross-validated MAE (optional)\n",
        "cv_mae = -cross_val_score(rf, train_embeddings, y_train.values, cv=5, scoring='neg_mean_absolute_error')\n",
        "print(\"RandomForest on Transformer Embeddings CV MAE:\", cv_mae)\n",
        "print(\"Mean MAE:\", np.mean(cv_mae))\n",
        "\n",
        "# Step 3: Predict on test set\n",
        "test_preds_rf = rf.predict(test_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "359c7396",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "def expanding_window_cv(model, X, y, \n",
        "                        initial_window=1000, \n",
        "                        horizon=200, \n",
        "                        step=200, \n",
        "                        verbose=True):\n",
        "    \"\"\"\n",
        "    Expanding window time series cross-validation.\n",
        "    \n",
        "    Args:\n",
        "        model: scikit-learn-like regressor (with fit/predict).\n",
        "        X: DataFrame or np.ndarray of features.\n",
        "        y: Series or np.ndarray of targets.\n",
        "        initial_window: Size of initial training window.\n",
        "        horizon: Size of each validation/test split.\n",
        "        step: How much to move forward each time.\n",
        "        verbose: Print split info and scores.\n",
        "        \n",
        "    Returns:\n",
        "        List of MAE scores for each fold.\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    cv_scores = []\n",
        "    folds = 0\n",
        "\n",
        "    for start in range(0, n - initial_window - horizon + 1, step):\n",
        "        train_end = start + initial_window\n",
        "        test_end = train_end + horizon\n",
        "\n",
        "        X_train = X.iloc[start:train_end]\n",
        "        y_train = y.iloc[start:train_end]\n",
        "        X_val = X.iloc[train_end:test_end]\n",
        "        y_val = y.iloc[train_end:test_end]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_val)\n",
        "        score = mean_absolute_error(y_val, preds)\n",
        "        cv_scores.append(score)\n",
        "        folds += 1\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[Fold {folds}] Train: {start}:{train_end}, Test: {train_end}:{test_end}, MAE: {score:.4f}\")\n",
        "\n",
        "    print(\"\\n Expanding Window CV Complete\")\n",
        "    print(f\"Folds run: {folds}\")\n",
        "    print(f\"MAE: mean={np.mean(cv_scores):.4f}, std={np.std(cv_scores):.4f}\")\n",
        "    return cv_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1df90ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV MAE scores: [8.135922330097088, 7.294019607843136, 7.232156862745098, 7.036274509803922, 7.384607843137253]\n",
            "Average MAE: 7.4165962307252995\n"
          ]
        }
      ],
      "source": [
        "print(\"CV MAE scores:\", cv_scores)\n",
        "print(\"Average MAE:\", np.mean(cv_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2b3f6f60",
      "metadata": {
        "id": "2b3f6f60",
        "outputId": "fbcdd781-3f7a-4556-a0d7-b8060598cfec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabularTransformer Train MAE: 22.622737884521484\n",
            "TabularTransformer Test predictions shape: (315,)\n",
            "RandomForest Embedding Train MAE: 5.2196673189823874\n",
            "RandomForest Embedding Test predictions shape: (315,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Compute final MAE on train and test sets for both models\n",
        "\n",
        "\n",
        "# For TabularTransformer predictions (test_preds)\n",
        "# Assume y_train_tensor is the true train targets, and test_preds is for test set\n",
        "\n",
        "# Train predictions (TabularTransformer)\n",
        "model.eval()\n",
        "train_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        pred = model(xb).squeeze().cpu().numpy()\n",
        "        train_preds.append(pred)\n",
        "train_preds = np.concatenate([p.reshape(-1) for p in train_preds])\n",
        "\n",
        "mae_train_tabular = mean_absolute_error(y_train_tensor, train_preds)\n",
        "print(\"TabularTransformer Train MAE:\", mae_train_tabular)\n",
        "\n",
        "# Test predictions (TabularTransformer)\n",
        "# test_preds already computed\n",
        "print(\"TabularTransformer Test predictions shape:\", test_preds.shape)\n",
        "\n",
        "# For RandomForest on Transformer Embeddings (test_preds_rf)\n",
        "# Train set\n",
        "rf_train_preds = rf.predict(train_embeddings)\n",
        "mae_train_rf = mean_absolute_error(y_train, rf_train_preds)\n",
        "print(\"RandomForest Embedding Train MAE:\", mae_train_rf)\n",
        "\n",
        "# Test set\n",
        "print(\"RandomForest Embedding Test predictions shape:\", test_preds_rf.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6b974c4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file saved as kaggle_submission.csv\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Create submission file\n",
        "submission = sample_submission_df.copy()\n",
        "submission['return_on_asset'] = submission['asset_id'].map(\n",
        "    dict(zip(test_agg.index, test_preds))\n",
        ").fillna(0)\n",
        "\n",
        "# Save the CSV file\n",
        "submission.to_csv(\"submission_simple_Temporal_randomF.csv\", index=False)\n",
        "print(\"Submission file saved as kaggle_submission.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
